{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195cdc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing colon in file PosixPath('/home/bongo/anaconda3/lib/python3.11/site-packages/matplotlib/mpl-data/matplotlibrc'), line 263 (' sans-serif')\n",
      "2025-10-08 14:17:55.079810: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-08 14:17:55.107810: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-08 14:17:55.107831: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-08 14:17:55.108651: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-08 14:17:55.113697: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-08 14:17:55.790483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized image processor in facebook/dinov3-vitb16-pretrain-lvd1689m. Should have a `image_processor_type` key in its preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: align, beit, bit, blip, blip-2, bridgetower, chinese_clip, clip, clipseg, conditional_detr, convnext, convnextv2, cvt, data2vec-vision, deformable_detr, deit, deta, detr, dinat, dinov2, donut-swin, dpt, efficientformer, efficientnet, flava, focalnet, git, glpn, groupvit, idefics, imagegpt, instructblip, layoutlmv2, layoutlmv3, levit, mask2former, maskformer, mgp-str, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, nat, oneformer, owlvit, perceiver, pix2struct, poolformer, pvt, regnet, resnet, sam, segformer, swiftformer, swin, swin2sr, swinv2, table-transformer, timesformer, tvlt, upernet, van, videomae, vilt, vit, vit_hybrid, vit_mae, vit_msn, xclip, yolos",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoImageProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/dinov3-vitb16-pretrain-lvd1689m\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/dinov3-vitb16-pretrain-lvd1689m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# dinov3-vitb16-pretrain-lvd1689m\u001b[39;00m\n\u001b[1;32m     14\u001b[0m patch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpatch_size\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:401\u001b[0m, in \u001b[0;36mAutoImageProcessor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m     image_processor_class \u001b[38;5;241m=\u001b[39m IMAGE_PROCESSOR_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_processor_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized image processor in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Should have a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`image_processor_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mIMAGE_PROCESSOR_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, or one of the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model_type` keys in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mIMAGE_PROCESSOR_MAPPING_NAMES\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized image processor in facebook/dinov3-vitb16-pretrain-lvd1689m. Should have a `image_processor_type` key in its preprocessor_config.json of config.json, or one of the following `model_type` keys in its config.json: align, beit, bit, blip, blip-2, bridgetower, chinese_clip, clip, clipseg, conditional_detr, convnext, convnextv2, cvt, data2vec-vision, deformable_detr, deit, deta, detr, dinat, dinov2, donut-swin, dpt, efficientformer, efficientnet, flava, focalnet, git, glpn, groupvit, idefics, imagegpt, instructblip, layoutlmv2, layoutlmv3, levit, mask2former, maskformer, mgp-str, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, nat, oneformer, owlvit, perceiver, pix2struct, poolformer, pvt, regnet, resnet, sam, segformer, swiftformer, swin, swin2sr, swinv2, table-transformer, timesformer, tvlt, upernet, van, videomae, vilt, vit, vit_hybrid, vit_mae, vit_msn, xclip, yolos"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers.image_utils import load_image\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov3-vitb16-pretrain-lvd1689m\", output_attentions=True)\n",
    "model = AutoModel.from_pretrained(\"facebook/dinov3-vitb16-pretrain-lvd1689m\").to(device)  # dinov3-vitb16-pretrain-lvd1689m\n",
    "patch_size = model.config.patch_size\n",
    "num_register_tokens = model.config.num_register_tokens\n",
    "\n",
    "# === Extract patch features from image ===\n",
    "def get_dino_features(image_path):\n",
    "    image = load_image(image_path)\n",
    "    original = image.resize((224, 224))\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    last_hidden = outputs.last_hidden_state\n",
    "    _, _, H, W = inputs.pixel_values.shape\n",
    "    num_patches_h, num_patches_w = H // patch_size, W // patch_size\n",
    "\n",
    "    patch_tokens = last_hidden[:, 1 + num_register_tokens:, :]\n",
    "    patch_grid = patch_tokens.unflatten(1, (num_patches_h, num_patches_w))\n",
    "    patch_grid = patch_grid.view(1, -1, 768)\n",
    "    return patch_grid, original.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec42beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_ground_truth( gt_path):\n",
    "    \"\"\"\n",
    "    Load and process ground truth image\n",
    "    Args:\n",
    "        gt_path (str): Path to the ground truth image\n",
    "    Returns:\n",
    "        torch.Tensor: Processed ground truth tensor normalized to [0, 1]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the ground truth image\n",
    "        gt_img = Image.open(gt_path)\n",
    "\n",
    "        # Convert to grayscale if image is RGB\n",
    "        if gt_img.mode == 'RGB':\n",
    "            gt_img = gt_img.convert('L')\n",
    "\n",
    "        # Convert to tensor\n",
    "        gt_tensor = transforms.ToTensor()(gt_img).squeeze(0)\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        if gt_tensor.max() > 0:\n",
    "            gt_tensor = (gt_tensor - gt_tensor.min()) / (gt_tensor.max() - gt_tensor.min())\n",
    "\n",
    "        return gt_tensor\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to load ground truth image: {str(e)}\")\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7cf848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import timm\n",
    "import torchvision.transforms as T\n",
    "from sklearn.cluster import KMeans\n",
    "from VLM_model_dot import  MetricsTracker\n",
    "\n",
    "import os\n",
    "from file_managing import (\n",
    "    load_selected_samples,\n",
    "    get_actual_path,\n",
    "    get_gt_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b80d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics( pred_heatmap, gt_map):\n",
    "    \"\"\"\n",
    "    Calculate comparison metrics between predicted heatmap and GT (following original metric.py)\n",
    "    Args:\n",
    "        pred_heatmap (torch.Tensor): Predicted heatmap\n",
    "        gt_map (torch.Tensor): Ground truth map\n",
    "    Returns:\n",
    "        dict: Dictionary containing KLD, SIM, and NSS metrics\n",
    "    \"\"\"\n",
    "    # Ensure inputs are proper tensors\n",
    "    if not isinstance(pred_heatmap, torch.Tensor):\n",
    "        pred_heatmap = torch.tensor(pred_heatmap)\n",
    "    if not isinstance(gt_map, torch.Tensor):\n",
    "        gt_map = torch.tensor(gt_map)\n",
    "\n",
    "    # Flatten tensors and add batch dimension for compatibility\n",
    "    pred = pred_heatmap.flatten().float().unsqueeze(0)  # [1, H*W]\n",
    "    gt = gt_map.flatten().float().unsqueeze(0)          # [1, H*W]\n",
    "\n",
    "    eps = 1e-10\n",
    "\n",
    "    # Calculate KLD following original implementation\n",
    "    # Normalize to probability distributions\n",
    "    pred_norm = pred / pred.sum(dim=1, keepdim=True)\n",
    "    gt_norm = gt / gt.sum(dim=1, keepdim=True)\n",
    "    pred_norm += eps\n",
    "    kld = F.kl_div(pred_norm.log(), gt_norm, reduction=\"batchmean\").item()\n",
    "\n",
    "    # Calculate SIM following original implementation\n",
    "    pred_sim = pred / pred.sum(dim=1, keepdim=True)\n",
    "    gt_sim = gt / gt.sum(dim=1, keepdim=True)\n",
    "    sim = torch.minimum(pred_sim, gt_sim).sum().item() / len(pred_sim)\n",
    "\n",
    "    # Calculate NSS following original implementation\n",
    "    # First normalize by max values\n",
    "    pred_nss = pred / pred.max(dim=1, keepdim=True).values\n",
    "    gt_nss = gt / gt.max(dim=1, keepdim=True).values\n",
    "\n",
    "    # Calculate z-score for prediction\n",
    "    std = pred_nss.std(dim=1, keepdim=True)\n",
    "    u = pred_nss.mean(dim=1, keepdim=True)\n",
    "    smap = (pred_nss - u) / (std + eps)\n",
    "\n",
    "    # Create fixation map from GT\n",
    "    fixation_map = (gt_nss - torch.min(gt_nss, dim=1, keepdim=True).values) / (\n",
    "        torch.max(gt_nss, dim=1, keepdim=True).values - torch.min(gt_nss, dim=1, keepdim=True).values + eps)\n",
    "    fixation_map = (fixation_map >= 0.1).float()\n",
    "\n",
    "    # Calculate NSS\n",
    "    nss_values = smap * fixation_map\n",
    "    nss = nss_values.sum(dim=1) / (fixation_map.sum(dim=1) + eps)\n",
    "    nss = nss.mean().item()\n",
    "\n",
    "    return {\n",
    "        'KLD': kld,\n",
    "        'SIM': sim,\n",
    "        'NSS': nss\n",
    "    }\n",
    "\n",
    "\n",
    "def load_vlm_heatmap(heatmap_path: str, target_size: tuple) -> np.ndarray:\n",
    "    \"\"\"저장된 VLM 히트맵을 불러옵니다.\"\"\"\n",
    "    heatmap_img = Image.open(heatmap_path).convert('L')\n",
    "    heatmap_img = heatmap_img.resize(target_size, resample=Image.Resampling.BILINEAR)\n",
    "    heatmap_array = np.array(heatmap_img).astype(np.float32) / 255.0\n",
    "    return heatmap_array\n",
    "\n",
    "def cluster_and_select_affordance(\n",
    "    dino_patch_tokens: torch.Tensor, \n",
    "    vlm_heatmap: np.ndarray, \n",
    "    original_image_size: tuple, \n",
    "    n_clusters: int = 5\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    DINO 특징을 클러스터링하고, VLM 히트맵을 이용해 affordance 클러스터를 선택합니다.\n",
    "    \"\"\"\n",
    "    features = dino_patch_tokens.squeeze(0).cpu().numpy() # 예: (1369, 768)\n",
    "\n",
    "    # ✨ 해결책: 패치 개수로부터 h, w를 동적으로 계산\n",
    "    num_patches = features.shape[0]\n",
    "    h = w = int(np.sqrt(num_patches))\n",
    "    \n",
    "    # --- 1. DINO 특징 벡터에 K-Means 클러스터링 적용 ---\n",
    "#     print(f\"Performing K-Means clustering with K={n_clusters}...\")\n",
    "    features = dino_patch_tokens.squeeze(0).cpu().numpy() # (196, 768)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init='auto').fit(features)\n",
    "    cluster_ids = kmeans.labels_ # 각 패치가 속한 클러스터 ID (0~4)\n",
    "    \n",
    "    cluster_map = cluster_ids.reshape(h, w) # (14, 14)\n",
    "\n",
    "    # --- 2. VLM 앵커 포인트가 속한 클러스터 ID 식별 ---\n",
    "    # VLM 히트맵에서 가장 값이 높은 지점(앵커)의 좌표를 찾습니다.\n",
    "    anchor_y_px, anchor_x_px = np.unravel_index(np.argmax(vlm_heatmap), vlm_heatmap.shape)\n",
    "    \n",
    "    # 앵커 좌표를 클러스터 맵 크기(14x14)에 맞게 스케일링\n",
    "    anchor_y_map = int(anchor_y_px / original_image_size[1] * h)\n",
    "    anchor_x_map = int(anchor_x_px / original_image_size[0] * w)\n",
    "    \n",
    "    # 앵커가 속한 클러스터 ID를 찾습니다.\n",
    "    target_cluster_id = cluster_map[anchor_y_map, anchor_x_map]\n",
    "#     print(f\"VLM anchor belongs to Cluster ID: {target_cluster_id}\")\n",
    "    \n",
    "    # --- 3. 최종 히트맵(마스크) 생성 ---\n",
    "    # 타겟 클러스터에 속하는 모든 픽셀을 1로, 나머지를 0으로 설정\n",
    "    final_mask_small = (cluster_map == target_cluster_id).astype(np.float32)\n",
    "    \n",
    "    # 원본 이미지 크기로 리사이즈하여 최종 히트맵 생성\n",
    "    final_heatmap = np.array(Image.fromarray(final_mask_small).resize(original_image_size, resample=Image.Resampling.NEAREST))\n",
    "    \n",
    "    return final_heatmap, cluster_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3080413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 123 samples...\n",
      "==================================================\n",
      "--- Start  1  / 123 --------------------------------------------------------------------------------\n",
      "skis jump skis_002829.jpg\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X1_selected' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m X1_normalized = (X1 - X1.min()) / (X1.max() - X1.min())\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# 정규화\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m X1_normalized = (\u001b[43mX1_selected\u001b[49m - X1_selected.min()) / (X1_selected.max() - X1_selected.min())\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# --- 시각화 ---\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# 전체 히트맵을 담을 빈 배열 생성\u001b[39;00m\n\u001b[32m     69\u001b[39m result_heatmap = np.zeros_like(flat_heatmap, dtype=\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X1_selected' is not defined"
     ]
    }
   ],
   "source": [
    "metrics_tracker_dino = MetricsTracker(name=\"only_ego\")\n",
    "\n",
    "json_path = os.path.join(\"selected_samples.json\")\n",
    "data = load_selected_samples(json_path)\n",
    "missing_gt = 0\n",
    "processed_count = 0\n",
    "\n",
    "# Get total number of samples\n",
    "total_samples = len(data['selected_samples'])\n",
    "\n",
    "# Process each sample\n",
    "print(f\"Processing {total_samples} samples...\")\n",
    "print(\"=\" * 50)    \n",
    "for pair_key, sample_info in data[\"selected_samples\"].items():\n",
    "    processed_count += 1\n",
    "    print(f\"--- Start  {processed_count}  / {total_samples}\", \"-\"*80) \n",
    "    original_image_path = sample_info['image_path'].replace(\"${AGD20K_PATH}\",'/home/DATA/AGD20K')\n",
    "    \n",
    "    file_name = os.path.basename(sample_info['image_path'])\n",
    "    action_name = sample_info['image_path'].split('/')[4]\n",
    "    if file_name.count(\"_\") ==1:\n",
    "        item_name = file_name.split(\"_\")[0]\n",
    "    else:\n",
    "        item_name = file_name.split(\"_\")[0] + \"_\" + file_name.split(\"_\")[1]\n",
    "    AGD20K_PATH = '/home/DATA/AGD20K'\n",
    "    vlm_heatmap_path = f\"/home/bongo/porter_notebook/research/new_qwen_AG/32B_dino_1_power2/heatmaps/{file_name.split('.')[0]}_{action_name}_heatmap.jpg\"\n",
    "    gt_path =  f\"{AGD20K_PATH}/Seen/testset/GT/{action_name}/{item_name}/{file_name.split('.')[0]}.png\"\n",
    "    dot_path = f\"/home/bongo/porter_notebook/research/new_qwen_AG/32B_dino_1_power2/dots_only//{file_name.split('.')[0]}_{action_name}_dots.jpg\"\n",
    "    print(item_name, action_name, file_name)\n",
    "    output_path = f\"dino_fusion/{file_name.split('.')[0]}_{action_name}.png\"\n",
    "    # --- 2. VLM 히트맵 로드 및 DINO 특징 추출 ---\n",
    "    original_image = Image.open(original_image_path).convert('RGB')\n",
    "    dot_image = Image.open(dot_path).convert('RGB')\n",
    "    vlm_heatmap_image =  Image.open(vlm_heatmap_path).convert('RGB')\n",
    "#     print(\"Loading VLM heatmap...\")\n",
    "    vlm_heatmap = load_vlm_heatmap(vlm_heatmap_path, original_image.size)\n",
    "\n",
    "    # PCA Guide\n",
    "    heatmap_4d = torch.from_numpy(vlm_heatmap).float().unsqueeze(0).unsqueeze(0)\n",
    "    guidance_heatmap_14x14 = F.adaptive_avg_pool2d(heatmap_4d, (14, 14)).squeeze()\n",
    "    # 2. 가이드 히트맵을 기반으로 마스크 생성\n",
    "    flat_heatmap = guidance_heatmap_14x14.flatten() # 1D로 변환 (196,)\n",
    "    threshold = 0.5 # 활성화 기준으로 삼을 임계값 (조정 필요)\n",
    "    mask = flat_heatmap > threshold\n",
    "\n",
    "\n",
    "#     print(\"Extracting DINO features...\")\n",
    "    dino_patch_tokens, _ = get_dino_features(original_image_path)\n",
    "\n",
    "    # 3. 마스크를 사용해 관심 영역의 패치만 선택\n",
    "    all_patches = dino_patch_tokens.cpu().reshape(-1, 768) # (196, 768)\n",
    "    selected_patches = all_patches[mask]\n",
    "\n",
    "\n",
    "    ## PCA !!!\n",
    "    # patch_grid가 (196, 768) 또는 (1, 196, 768) 모양이라고 가정\n",
    "    X = dino_patch_tokens.cpu().reshape(-1, 768).numpy()\n",
    "    # 1️⃣ n_components를 1로 변경\n",
    "    pca = PCA(n_components=1)\n",
    "    X1 = pca.fit_transform(selected_patches) # 이제 X1의 shape은 (196, 1)이 됩니다.\n",
    "    X1_normalized = (X1 - X1.min()) / (X1.max() - X1.min())\n",
    "\n",
    "    # --- 시각화 ---\n",
    "    # 전체 히트맵을 담을 빈 배열 생성\n",
    "    heatmap = np.zeros_like(flat_heatmap, dtype=float)\n",
    "    \n",
    "    # 선택된 위치에만 PCA 결과 값을 다시 채워 넣기\n",
    "    heatmap[mask] = X1_normalized.flatten()\n",
    "    \n",
    "\n",
    "    # 1D 맵을 2D 히트맵 형태로 변경\n",
    "    # heatmap = X1_normalized.reshape(14, 14)\n",
    "    # np.kron을 사용해 히트맵 확대\n",
    "    dino_attention_heatmap =np.array(Image.fromarray(heatmap).resize(original_image.size, resample=Image.Resampling.BILINEAR))\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     features = dino_patch_tokens.squeeze(0).cpu().numpy() # 예: (1369, 768)\n",
    "\n",
    "    #     # ✨ 해결책: 패치 개수로부터 h, w를 동적으로 계산\n",
    "    #     num_patches = features.shape[0]\n",
    "    #     h = w = int(np.sqrt(num_patches))\n",
    "\n",
    "    #     # 패치 토큰의 norm을 사용하여 어텐션 맵 계산\n",
    "    #     attn_map = torch.norm(dino_patch_tokens, dim=-1).reshape(h, w)\n",
    "    #     # 0~1 범위로 정규화\n",
    "    #     attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())\n",
    "    #     # 원본 이미지 크기로 리사이즈 후 1에서 빼서 값을 반전시킴\n",
    "    #     dino_attention_heatmap = np.array(Image.fromarray(attn_map.squeeze(0).cpu().numpy()).resize(original_image.size, resample=Image.Resampling.BILINEAR))\n",
    "\n",
    "\n",
    "    vlm_heatmap = vlm_heatmap + vlm_heatmap.mean()*0.75\n",
    "    weighted_dino_heatmap = dino_attention_heatmap\n",
    "    vlm_fused_heatmap = vlm_heatmap * weighted_dino_heatmap\n",
    "    \n",
    "\n",
    "    \n",
    "    # Calculate metrics if GT is available\n",
    "    metrics = None\n",
    "    gt_map = load_ground_truth(gt_path)\n",
    "    if gt_map is not None:\n",
    "        metrics_dino  = calculate_metrics(vlm_fused_heatmap, gt_map)\n",
    "        metrics_tracker_dino.update(metrics_dino)\n",
    "    else:\n",
    "        print(\"NO GT!!!\")\n",
    "        continue\n",
    "    metrics_tracker_dino.print_metrics(metrics_dino, vlm_heatmap_path.split('/')[-1])\n",
    "    \n",
    "    # --- 4. 결과 시각화 ---\n",
    "    # ✨ 레이아웃을 1x4에서 1x5로 변경하고, figsize을 조정합니다.\n",
    "    fig, ax = plt.subplots(1, 6, figsize=(25, 5))\n",
    "\n",
    "    # --- Plot 1: 원본 이미지 (ax[0]) ---\n",
    "    ax[0].imshow(original_image)\n",
    "    ax[0].set_title('Original Image')\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    # --- ✨ Plot 5: 최종 퓨전 히트맵 (기존 ax[3] -> ax[4]로 이동) ---\n",
    "    ax[1].imshow(dot_image)\n",
    "    ax[1].set_title('Dot image')\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    # --- Plot 2: dot 히트맵 (ax[1]) ---\n",
    "    ax[2].imshow(original_image)\n",
    "    ax[2].imshow(vlm_heatmap_image , cmap='jet', alpha=0.5)\n",
    "    ax[2].set_title('dot (Input)')\n",
    "    ax[2].axis('off')\n",
    "\n",
    "    # --- Plot 2: VLM 히트맵 (ax[1]) ---\n",
    "    ax[3].imshow(original_image)\n",
    "    ax[3].imshow(vlm_fused_heatmap, cmap='jet', alpha=0.5)\n",
    "    ax[3].set_title('VLM Heatmap (Input)')\n",
    "    ax[3].axis('off')\n",
    "    \n",
    "    # --- ✨ Plot 3: DINO 원본 히트맵 (새로 추가된 부분) ---\n",
    "    # 이 dino_attention_heatmap 변수는 클러스터링 전에 미리 계산해 두어야 합니다.\n",
    "    # (예: dino_attention_heatmap = generate_dino_heatmap(original_image_path, dino_model) )\n",
    "    ax[4].imshow(original_image)\n",
    "    ax[4].imshow(dino_attention_heatmap, cmap='jet', alpha=0.5)\n",
    "    ax[4].set_title('DINO Heatmap (Attention)')\n",
    "    ax[4].axis('off')\n",
    "\n",
    "\n",
    "    # --- ✨ Plot 5: 최종 퓨전 히트맵 (기존 ax[3] -> ax[4]로 이동) ---\n",
    "    ax[5].imshow(original_image)\n",
    "    ax[5].imshow(gt_map, cmap='jet', alpha=0.5)\n",
    "    ax[5].set_title('GT')\n",
    "    ax[5].axis('off')\n",
    "#     전체 레이아웃 정리 및 출력\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # # --- 5. 최종 퓨전 히트맵 이미지로 저장 ---\n",
    "    # fused_heatmap_img = Image.fromarray((fused_heatmap * 255).astype(np.uint8))\n",
    "    # fused_heatmap_img.save(output_path)\n",
    "    # print(f\"Fused heatmap saved to {output_path}\")\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "168c6bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aa76748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(54)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed2f6be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(all_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f525b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(selected_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5f946c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X1_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4592182",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit b 16 result\n",
    "Cumulative only_ego  Averages over 121 samples:\n",
    "Average - KLD: 1.3131 | SIM: 0.3883 | NSS: 1.1577\n",
    "=================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov3",
   "language": "python",
   "name": "dinov3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
