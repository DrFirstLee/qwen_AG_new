{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa450d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bongo/anaconda3/envs/qwen25/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import my_prompt5_MA as my_prompt\n",
    "from file_managing import (\n",
    "    load_selected_samples,\n",
    "    get_actual_path,\n",
    "    get_gt_path,\n",
    ")\n",
    "from config import AGD20K_PATH, model_name\n",
    "from VLM_model_dot import QwenVLModel, MetricsTracker\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"PYTORCH_ENABLE_SDPA\"] = \"1\"\n",
    "\n",
    "AGD20K_VERBS = [\n",
    "    \"beat\", \"brush_with\", \"catch\", \"cut_with\", \"drink_with\", \"hit\", \"jump\", \"lie_on\", \"look_out\", \"pack\",\n",
    "    \"pick_up\", \"push\", \"sip\", \"stick\", \"swing\", \"talk_on\", \"throw\", \"wash\",\n",
    "    \"boxing\", \"carry\", \"cut\", \"drag\", \"eat\", \"hold\", \"kick\", \"lift\", \"open\", \"peel\", \"pour\", \"ride\",\n",
    "    \"sit_on\", \"stir\", \"take_photo\", \"text_on\", \"type_on\", \"write\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4f751",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Qwen3VLForConditionalGeneration' from 'transformers' (/home/bongo/anaconda3/envs/qwen25/lib/python3.9/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[185], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Qwen3VLForConditionalGeneration, AutoProcessor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# default: Load the model on the available device(s)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m Qwen3VLForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen3-VL-2B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Qwen3VLForConditionalGeneration' from 'transformers' (/home/bongo/anaconda3/envs/qwen25/lib/python3.9/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298566c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda\n",
      "ğŸ¤– Qwen/Qwen2.5-VL-3B-Instruct ëª¨ë¸ ë¡œë”©ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "model_name = model_name\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "# ëª¨ë¸ê³¼ processor ë¡œë“œ\n",
    "print(f\"ğŸ¤– {model_name} ëª¨ë¸ ë¡œë”©ì¤‘...\")\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_name\n",
    "            , torch_dtype=\"auto\"\n",
    "            , device_map=\"auto\"\n",
    "            ,trust_remote_code=True\n",
    "            )\n",
    "else:\n",
    "    print(\"âš ï¸  CPUë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...\")\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32\n",
    "    ).to(device)\n",
    "    \n",
    "print(\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33853518",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGD20K_VERBS = [\n",
    "    \"beat\", \"brush_with\", \"catch\", \"cut_with\", \"drink_with\", \"hit\", \"jump\", \"lie_on\", \"look_out\", \"pack\",\n",
    "    \"pick_up\", \"push\", \"sip\", \"stick\", \"swing\", \"talk_on\", \"throw\", \"wash\",\n",
    "    \"boxing\", \"carry\", \"cut\", \"drag\", \"eat\", \"hold\", \"kick\", \"lift\", \"open\", \"peel\", \"pour\", \"ride\",\n",
    "    \"sit_on\", \"stir\", \"take_photo\", \"text_on\", \"type_on\", \"write\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "549ba36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/bongo/porter_notebook/research/WSAG-PLSP/AGD20K/Seen/trainset/exocentric/brush_with/toothbrush/brush_with_toothbrush_000011.jpg'\n",
    "\n",
    "\n",
    "image_path = \"/home/bongo/porter_notebook/research/WSAG-PLSP/AGD20K/Seen/trainset/exocentric/push/bicycle/push_bicycle_007122.jpg\"\n",
    "\n",
    "# image_path = \"/home/bongo/porter_notebook/research/WSAG-PLSP/AGD20K/Seen/trainset/exocentric/push/bicycle/push_bicycle_012603.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ced7dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_item = 'bicycle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d528ddb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7057850360870361"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\"Is the person [action]?\" ì§ˆë¬¸ì— ëŒ€í•´ \n",
    "ëª¨ë¸ì´ 'Yes'ë¼ê³  ëŒ€ë‹µí•  í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "action_text = 'push'\n",
    "# 1. ì§ˆë¬¸ êµ¬ì„±: ëª…í™•í•˜ê²Œ Yes/No ëŒ€ë‹µì„ ìœ ë„\n",
    "# Qwen í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ë§ì¶° ì§ˆë¬¸\n",
    "query = f\"Is the person {action_text} {target_item} in this image? Answer with Yes or No.\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\", \"text\": query}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 2. ì…ë ¥ ì²˜ë¦¬\n",
    "text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text_input],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# 3. Forward Pass (Generate ì•„ë‹˜)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # 4. ë§ˆì§€ë§‰ í† í°ì˜ Logits ì¶”ì¶œ\n",
    "    # (ëª¨ë¸ì´ ë§‰ 'Yes' í˜¹ì€ 'No'ë¥¼ ë§í•˜ë ¤ëŠ” ìˆœê°„ì˜ í™•ë¥  ë¶„í¬)\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    \n",
    "    # 5. 'Yes'ì™€ 'No' í† í°ì˜ ID ì°¾ê¸°\n",
    "    # ëª¨ë¸ë§ˆë‹¤ í† í° IDê°€ ë‹¤ë¥´ë¯€ë¡œ tokenizerì—ì„œ ì°¾ì•„ì•¼ í•¨\n",
    "    # Qwen2.5ëŠ” ë³´í†µ \"Yes\", \"No\" (ëŒ€ì†Œë¬¸ì ìœ ì˜)\n",
    "    yes_token_id = processor.tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n",
    "    no_token_id = processor.tokenizer.encode(\"No\", add_special_tokens=False)[0]\n",
    "    \n",
    "    # 6. Yes vs No ì ìˆ˜ ë¹„êµ (Softmax)\n",
    "    # ì „ì²´ ë‹¨ì–´ì¥(vocab) ë§ê³ , ì˜¤ì§ Yesì™€ No ë‘ ê°œ ì‚¬ì´ì˜ í™•ë¥ ë§Œ ë´…ë‹ˆë‹¤.\n",
    "    yes_score = next_token_logits[0, yes_token_id].item()\n",
    "    no_score = next_token_logits[0, no_token_id].item()\n",
    "    \n",
    "    # Yesì¼ í™•ë¥  = exp(Yes) / (exp(Yes) + exp(No))\n",
    "    yes_prob = torch.softmax(torch.tensor([yes_score, no_score]), dim=0)[0].item()\n",
    "yes_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fe64cb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model output: 'yes.'\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=12,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        return_dict_in_generate =True,\n",
    "        output_scores=True,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# ìƒì„±ëœ í…ìŠ¤íŠ¸\n",
    "generated = processor.batch_decode(output.sequences[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0].strip().lower()\n",
    "print(f\"Raw model output: '{generated}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d438d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Yes/No í™•ë¥  ë¶„ì„ ì¤‘...\n",
      "\n",
      "ğŸ† Top 3 Predictions:\n",
      "Checking ride... Probability: 95.26%\n",
      "Checking sip... Probability: 90.47%\n",
      "Checking carry... Probability: 90.47%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "def get_yes_no_score(image_path, action_text,target_item):\n",
    "    \"\"\"\n",
    "    \"Is the person [action]?\" ì§ˆë¬¸ì— ëŒ€í•´ \n",
    "    ëª¨ë¸ì´ 'Yes'ë¼ê³  ëŒ€ë‹µí•  í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. ì§ˆë¬¸ êµ¬ì„±: ëª…í™•í•˜ê²Œ Yes/No ëŒ€ë‹µì„ ìœ ë„\n",
    "    # Qwen í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ë§ì¶° ì§ˆë¬¸\n",
    "    query = f\"Is the person {action_text} {target_item} in this image? Answer with Yes or No.\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path},\n",
    "                {\"type\": \"text\", \"text\": query}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 2. ì…ë ¥ ì²˜ë¦¬\n",
    "    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text_input],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # 3. Forward Pass (Generate ì•„ë‹˜)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # 4. ë§ˆì§€ë§‰ í† í°ì˜ Logits ì¶”ì¶œ\n",
    "        # (ëª¨ë¸ì´ ë§‰ 'Yes' í˜¹ì€ 'No'ë¥¼ ë§í•˜ë ¤ëŠ” ìˆœê°„ì˜ í™•ë¥  ë¶„í¬)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        # 5. 'Yes'ì™€ 'No' í† í°ì˜ ID ì°¾ê¸°\n",
    "        # ëª¨ë¸ë§ˆë‹¤ í† í° IDê°€ ë‹¤ë¥´ë¯€ë¡œ tokenizerì—ì„œ ì°¾ì•„ì•¼ í•¨\n",
    "        # Qwen2.5ëŠ” ë³´í†µ \"Yes\", \"No\" (ëŒ€ì†Œë¬¸ì ìœ ì˜)\n",
    "        yes_token_id = processor.tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n",
    "        no_token_id = processor.tokenizer.encode(\"No\", add_special_tokens=False)[0]\n",
    "        \n",
    "        # 6. Yes vs No ì ìˆ˜ ë¹„êµ (Softmax)\n",
    "        # ì „ì²´ ë‹¨ì–´ì¥(vocab) ë§ê³ , ì˜¤ì§ Yesì™€ No ë‘ ê°œ ì‚¬ì´ì˜ í™•ë¥ ë§Œ ë´…ë‹ˆë‹¤.\n",
    "        yes_score = next_token_logits[0, yes_token_id].item()\n",
    "        no_score = next_token_logits[0, no_token_id].item()\n",
    "        \n",
    "        # Yesì¼ í™•ë¥  = exp(Yes) / (exp(Yes) + exp(No))\n",
    "        yes_prob = torch.softmax(torch.tensor([yes_score, no_score]), dim=0)[0].item()\n",
    "        \n",
    "    return yes_prob\n",
    "\n",
    "# --- ì‹¤í–‰ ---\n",
    "print(\"ğŸ” Yes/No í™•ë¥  ë¶„ì„ ì¤‘...\")\n",
    "results = {}\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ ê²½ë¡œ\n",
    "# image_path = \"...\" \n",
    "\n",
    "for action in AGD20K_VERBS:\n",
    "    # action í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë³€í˜• (í•„ìš”ì‹œ)\n",
    "    # ì˜ˆ: \"jumping\", \"holding a cup\" ë“± í˜„ì¬ì§„í–‰í˜• ê¶Œì¥\n",
    "    score = get_yes_no_score(image_path, action, target_item = 'bicyle')\n",
    "    results[action] = score\n",
    "    # print(f\"{action}: {score:.4f}\")\n",
    "\n",
    "# Top-3 ê²°ê³¼ ì¶œë ¥\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nğŸ† Top 3 Predictions:\")\n",
    "for act, prob in sorted_results[:3]:\n",
    "    print(f\"Checking {act}... Probability: {prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "952c0f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ride</th>\n",
       "      <td>0.952574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carry</th>\n",
       "      <td>0.904651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sip</th>\n",
       "      <td>0.904651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pour</th>\n",
       "      <td>0.893309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_on</th>\n",
       "      <td>0.893309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write</th>\n",
       "      <td>0.880797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>throw</th>\n",
       "      <td>0.880797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brush_with</th>\n",
       "      <td>0.851953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eat</th>\n",
       "      <td>0.851953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hold</th>\n",
       "      <td>0.835484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_on</th>\n",
       "      <td>0.835484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cut_with</th>\n",
       "      <td>0.817574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stir</th>\n",
       "      <td>0.817574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catch</th>\n",
       "      <td>0.777300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look_out</th>\n",
       "      <td>0.777300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.777300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take_photo</th>\n",
       "      <td>0.754915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beat</th>\n",
       "      <td>0.705785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>0.679179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stick</th>\n",
       "      <td>0.679179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wash</th>\n",
       "      <td>0.651355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pick_up</th>\n",
       "      <td>0.622459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sit_on</th>\n",
       "      <td>0.622459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drink_with</th>\n",
       "      <td>0.592667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit</th>\n",
       "      <td>0.592667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swing</th>\n",
       "      <td>0.531209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peel</th>\n",
       "      <td>0.531209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drag</th>\n",
       "      <td>0.531209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boxing</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cut</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>push</th>\n",
       "      <td>0.437824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lift</th>\n",
       "      <td>0.407333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kick</th>\n",
       "      <td>0.348645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk_on</th>\n",
       "      <td>0.294215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lie_on</th>\n",
       "      <td>0.201813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jump</th>\n",
       "      <td>0.182426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "ride        0.952574\n",
       "carry       0.904651\n",
       "sip         0.904651\n",
       "pour        0.893309\n",
       "type_on     0.893309\n",
       "write       0.880797\n",
       "throw       0.880797\n",
       "brush_with  0.851953\n",
       "eat         0.851953\n",
       "hold        0.835484\n",
       "text_on     0.835484\n",
       "cut_with    0.817574\n",
       "stir        0.817574\n",
       "catch       0.777300\n",
       "look_out    0.777300\n",
       "pack        0.777300\n",
       "take_photo  0.754915\n",
       "beat        0.705785\n",
       "open        0.679179\n",
       "stick       0.679179\n",
       "wash        0.651355\n",
       "pick_up     0.622459\n",
       "sit_on      0.622459\n",
       "drink_with  0.592667\n",
       "hit         0.592667\n",
       "swing       0.531209\n",
       "peel        0.531209\n",
       "drag        0.531209\n",
       "boxing      0.500000\n",
       "cut         0.500000\n",
       "push        0.437824\n",
       "lift        0.407333\n",
       "kick        0.348645\n",
       "talk_on     0.294215\n",
       "lie_on      0.201813\n",
       "jump        0.182426"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([results]).T.sort_values(by=0,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2339d473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ride</th>\n",
       "      <td>0.952574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carry</th>\n",
       "      <td>0.904651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sip</th>\n",
       "      <td>0.904651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pour</th>\n",
       "      <td>0.893309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_on</th>\n",
       "      <td>0.893309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write</th>\n",
       "      <td>0.880797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>throw</th>\n",
       "      <td>0.880797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brush_with</th>\n",
       "      <td>0.851953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eat</th>\n",
       "      <td>0.851953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hold</th>\n",
       "      <td>0.835484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_on</th>\n",
       "      <td>0.835484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cut_with</th>\n",
       "      <td>0.817574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stir</th>\n",
       "      <td>0.817574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catch</th>\n",
       "      <td>0.777300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look_out</th>\n",
       "      <td>0.777300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>0.777300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take_photo</th>\n",
       "      <td>0.754915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beat</th>\n",
       "      <td>0.705785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>0.679179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stick</th>\n",
       "      <td>0.679179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wash</th>\n",
       "      <td>0.651355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pick_up</th>\n",
       "      <td>0.622459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sit_on</th>\n",
       "      <td>0.622459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drink_with</th>\n",
       "      <td>0.592667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit</th>\n",
       "      <td>0.592667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swing</th>\n",
       "      <td>0.531209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peel</th>\n",
       "      <td>0.531209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drag</th>\n",
       "      <td>0.531209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boxing</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cut</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>push</th>\n",
       "      <td>0.437824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lift</th>\n",
       "      <td>0.407333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kick</th>\n",
       "      <td>0.348645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk_on</th>\n",
       "      <td>0.294215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lie_on</th>\n",
       "      <td>0.201813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jump</th>\n",
       "      <td>0.182426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "ride        0.952574\n",
       "carry       0.904651\n",
       "sip         0.904651\n",
       "pour        0.893309\n",
       "type_on     0.893309\n",
       "write       0.880797\n",
       "throw       0.880797\n",
       "brush_with  0.851953\n",
       "eat         0.851953\n",
       "hold        0.835484\n",
       "text_on     0.835484\n",
       "cut_with    0.817574\n",
       "stir        0.817574\n",
       "catch       0.777300\n",
       "look_out    0.777300\n",
       "pack        0.777300\n",
       "take_photo  0.754915\n",
       "beat        0.705785\n",
       "open        0.679179\n",
       "stick       0.679179\n",
       "wash        0.651355\n",
       "pick_up     0.622459\n",
       "sit_on      0.622459\n",
       "drink_with  0.592667\n",
       "hit         0.592667\n",
       "swing       0.531209\n",
       "peel        0.531209\n",
       "drag        0.531209\n",
       "boxing      0.500000\n",
       "cut         0.500000\n",
       "push        0.437824\n",
       "lift        0.407333\n",
       "kick        0.348645\n",
       "talk_on     0.294215\n",
       "lie_on      0.201813\n",
       "jump        0.182426"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([results]).T.sort_values(by=0,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949f661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d18d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e348ea62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beat: -17.785367965698242\n",
      "brush_with: -17.79235076904297\n",
      "catch: -17.787118911743164\n",
      "cut_with: -17.789731979370117\n",
      "drink_with: -17.78794288635254\n",
      "hit: -17.78059196472168\n",
      "jump: -17.78775978088379\n",
      "lie_on: -17.778459548950195\n",
      "look_out: -17.761341094970703\n",
      "pack: -17.78988265991211\n",
      "pick_up: -17.761762619018555\n",
      "push: -17.774559020996094\n",
      "sip: -17.794403076171875\n",
      "stick: -17.77835464477539\n",
      "swing: -17.7894344329834\n",
      "talk_on: -17.776988983154297\n",
      "throw: -17.791954040527344\n",
      "wash: -17.791332244873047\n",
      "boxing: -17.771692276000977\n",
      "carry: -17.784887313842773\n",
      "cut: -17.7902774810791\n",
      "drag: -17.78997802734375\n",
      "eat: -17.784038543701172\n",
      "hold: -17.783693313598633\n",
      "kick: -17.791017532348633\n",
      "lift: -17.782751083374023\n",
      "open: -17.778596878051758\n",
      "peel: -17.78714942932129\n",
      "pour: -17.787384033203125\n",
      "ride: -17.770360946655273\n",
      "sit_on: -17.76946258544922\n",
      "stir: -17.79750633239746\n",
      "take_photo: -17.755605697631836\n",
      "text_on: -17.77385902404785\n",
      "type_on: -17.765336990356445\n",
      "write: -17.78521728515625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "def get_action_score_masked(image_path, action_text):\n",
    "    \"\"\"\n",
    "    ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ ë¶€ë¶„ì€ Loss ê³„ì‚°ì—ì„œ ì œì™¸(-100)í•˜ê³ ,\n",
    "    ì˜¤ì§ 'í–‰ë™ í…ìŠ¤íŠ¸(Answer)' ë¶€ë¶„ì˜ ìƒì„± í™•ë¥ ë§Œ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. [Prompt] ì§ˆë¬¸ê¹Œì§€ë§Œ ë§Œë“¤ê¸° (ì´ë¯¸ì§€ í¬í•¨)\n",
    "    prompt_messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path},\n",
    "                {\"type\": \"text\", \"text\": \"Describe the action in the image.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 2. [Full] ì§ˆë¬¸ + ì •ë‹µê¹Œì§€ ë§Œë“¤ê¸°\n",
    "    # ì—¬ê¸°ì„œ action_textë¥¼ ì¢€ ë” ë¬¸ì¥ë‹µê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ê²Œ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "    # ì˜ˆ: \"jump\" -> \"The person is jumping.\"\n",
    "    answer_text = f\"The person is {action_text}.\" \n",
    "    \n",
    "    full_messages = prompt_messages + [\n",
    "        {\"role\": \"assistant\", \"content\": answer_text}\n",
    "    ]\n",
    "    \n",
    "    # 3. í† í¬ë‚˜ì´ì§• (ê¸¸ì´ ê³„ì‚°ì„ ìœ„í•´ ë”°ë¡œ ìˆ˜í–‰)\n",
    "    # A. í”„ë¡¬í”„íŠ¸(ì§ˆë¬¸) ë¶€ë¶„ ê¸¸ì´ êµ¬í•˜ê¸°\n",
    "    prompt_text = processor.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
    "    prompt_inputs = processor(text=[prompt_text], return_tensors=\"pt\")\n",
    "    len_prompt = prompt_inputs.input_ids.shape[1]\n",
    "    \n",
    "    # B. ì „ì²´(ì§ˆë¬¸+ë‹µë³€) ì…ë ¥ ë§Œë“¤ê¸°\n",
    "    full_text = processor.apply_chat_template(full_messages, tokenize=False, add_generation_prompt=False)\n",
    "    image_inputs, video_inputs = process_vision_info(full_messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[full_text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # 4. [í•µì‹¬!] Labels ë§ˆìŠ¤í‚¹ (Masking)\n",
    "    # ë¨¼ì € ì „ì²´ ì…ë ¥ì„ ì •ë‹µì§€ë¡œ ë³µì‚¬\n",
    "    targets = inputs.input_ids.clone()\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸(ì§ˆë¬¸+ì´ë¯¸ì§€) ê¸¸ì´ë§Œí¼ì€ -100ìœ¼ë¡œ ë®ì–´ì¨ì„œ Loss ê³„ì‚° ì œì™¸\n",
    "    # (ì£¼ì˜: Qwen í† í¬ë‚˜ì´ì € íŠ¹ì„±ì— ë”°ë¼ 1~2í† í° ì˜¤ì°¨ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë‚˜, ëŒ€ëµì ìœ¼ë¡œ ë§ìŠµë‹ˆë‹¤)\n",
    "    # ì•ˆì „í•˜ê²Œ len_promptë³´ë‹¤ ì•½ê°„ ì§§ê²Œ ì¡ì•„ë„ ë˜ì§€ë§Œ, ë³´í†µì€ ê·¸ëŒ€ë¡œ ì”ë‹ˆë‹¤.\n",
    "    targets[:, :len_prompt] = -100 \n",
    "    \n",
    "    # íŒ¨ë”© í† í°ë„ -100 ì²˜ë¦¬ (ì´ë¯¸ padding sideì— ë”°ë¼ ì²˜ë¦¬ë˜ì–´ìˆì„ ìˆ˜ ìˆìŒ)\n",
    "    if processor.tokenizer.pad_token_id is not None:\n",
    "        targets[targets == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    # 5. Loss ê³„ì‚°\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=targets)\n",
    "        \n",
    "        # ì´ì œ lossëŠ” ìˆœìˆ˜í•˜ê²Œ 'answer_text' ë¶€ë¶„ë§Œ ë°˜ì˜í•©ë‹ˆë‹¤.\n",
    "        # í† í° ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ì •ê·œí™”(Normalization)í•˜ë©´ ë” ì •í™•í•©ë‹ˆë‹¤.\n",
    "        # (lossëŠ” í‰ê· ê°’ì´ë¯€ë¡œ ì´ë¯¸ ì •ê·œí™” íš¨ê³¼ê°€ ìˆì§€ë§Œ, ê¸¸ì´ ì°¨ì´ê°€ í¬ë‹¤ë©´ ê³ ë ¤)\n",
    "        loss = outputs.loss.item()\n",
    "        score = -loss \n",
    "\n",
    "    return score\n",
    "\n",
    "# --- ì‹¤í–‰ ---\n",
    "results = {}\n",
    "for action in AGD20K_VERBS:\n",
    "   score = get_action_score_masked(image_path, action)\n",
    "   print(f\"{action}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a7cb7f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” í–‰ë™ ë¶„ì„ ì¤‘...\n",
      "Action: beat, Score: -17.6591\n",
      "Action: brush_with, Score: -17.6675\n",
      "Action: catch, Score: -17.6549\n",
      "Action: cut_with, Score: -17.6651\n",
      "Action: drink_with, Score: -17.6557\n",
      "Action: hit, Score: -17.6767\n",
      "Action: jump, Score: -17.6468\n",
      "Action: lie_on, Score: -17.6495\n",
      "Action: look_out, Score: -17.6443\n",
      "Action: pack, Score: -17.6588\n",
      "Action: pick_up, Score: -17.6350\n",
      "Action: push, Score: -17.6646\n",
      "Action: sip, Score: -17.6632\n",
      "Action: stick, Score: -17.6580\n",
      "Action: swing, Score: -17.6495\n",
      "Action: talk_on, Score: -17.6706\n",
      "Action: throw, Score: -17.6646\n",
      "Action: wash, Score: -17.6490\n",
      "Action: boxing, Score: -17.6532\n",
      "Action: carry, Score: -17.6544\n",
      "Action: cut, Score: -17.6582\n",
      "Action: drag, Score: -17.6551\n",
      "Action: eat, Score: -17.6477\n",
      "Action: hold, Score: -17.6491\n",
      "Action: kick, Score: -17.6515\n",
      "Action: lift, Score: -17.6571\n",
      "Action: open, Score: -17.6595\n",
      "Action: peel, Score: -17.6091\n",
      "Action: pour, Score: -17.6474\n",
      "Action: ride, Score: -17.6453\n",
      "Action: sit_on, Score: -17.6482\n",
      "Action: stir, Score: -17.6200\n",
      "Action: take_photo, Score: -17.6282\n",
      "Action: text_on, Score: -17.6528\n",
      "Action: type_on, Score: -17.6452\n",
      "Action: write, Score: -17.6484\n",
      "\n",
      "ğŸ† ì •ë‹µ ì˜ˆìƒ: peel (Score: -17.6091)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "def get_action_score(image_path, full_text):\n",
    "    \"\"\"\n",
    "    ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸(ì˜ˆ: \"ride a bicycle\")ê°€ ì£¼ì–´ì¡Œì„ ë•Œ,\n",
    "    ëª¨ë¸ì´ ì´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  í™•ë¥ (Score)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    Lossê°€ ë‚®ì„ìˆ˜ë¡ Scoreê°€ ë†’ìŠµë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # 1. ë©”ì‹œì§€ êµ¬ì„±\n",
    "    # Qwenì—ê²Œ ì´ë¯¸ì§€ë¥¼ ë³´ì—¬ì£¼ê³ , ìš°ë¦¬ê°€ ê²€ì¦í•˜ê³  ì‹¶ì€ ë¬¸ì¥(full_text)ì„ ì •ë‹µìœ¼ë¡œ ì¤ë‹ˆë‹¤.\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path},\n",
    "                {\"type\": \"text\", \"text\": \"Describe the action in the image.\"} # ì§ˆë¬¸\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": full_text  # ê²€ì¦í•  ì •ë‹µ (ì˜ˆ: \"The person is riding a bicycle.\")\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 2. ì…ë ¥ ì²˜ë¦¬ (process_vision_info í•„ìˆ˜)\n",
    "    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text_input],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # 3. Loss ê³„ì‚°\n",
    "    with torch.no_grad():\n",
    "        # labelsì— input_idsë¥¼ ê·¸ëŒ€ë¡œ ë„£ì–´ì£¼ë©´, ëª¨ë¸ì´ \"User ì§ˆë¬¸ + Image\"ë¥¼ ë³´ê³  \"Assistant ë‹µë³€\"ì„ ì˜ˆì¸¡í•˜ëŠ” Lossë¥¼ ê³„ì‚°í•¨\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        \n",
    "        # LossëŠ” 'í‹€ë¦° ì •ë„'ì´ë¯€ë¡œ, ë‚®ì„ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "        # ë¹„êµë¥¼ ìœ„í•´ ìŒìˆ˜ë¥¼ ì·¨í•´ 'ì ìˆ˜'ë¡œ ë§Œë“­ë‹ˆë‹¤. (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)\n",
    "        loss = outputs.loss.item()\n",
    "        score = -loss \n",
    "        \n",
    "    return score\n",
    "\n",
    "# --- ì‹¤ì œ ì‚¬ìš© ---\n",
    "image_path\n",
    "results = {}\n",
    "\n",
    "print(\"ğŸ” í–‰ë™ ë¶„ì„ ì¤‘...\")\n",
    "\n",
    "for action in AGD20K_VERBS:\n",
    "    # ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒì´ LLMì—ê²Œ ë” ì¢‹ìŠµë‹ˆë‹¤.\n",
    "    phrase = f\"The person is riding the bicycle by placing both feet on the pedals and pushing forward\" \n",
    "    \n",
    "    # ì„ë² ë”© ë‚´ì  ëŒ€ì‹  Score ê³„ì‚°\n",
    "    score = get_action_score(image_path, action)\n",
    "    results[action] = score\n",
    "    print(f\"Action: {action}, Score: {score:.4f}\")\n",
    "\n",
    "# ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ Action ì°¾ê¸°\n",
    "best_action = max(results, key=results.get)\n",
    "print(f\"\\nğŸ† ì •ë‹µ ì˜ˆìƒ: {best_action} (Score: {results[best_action]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2e09da03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beat</th>\n",
       "      <td>-17.659121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brush_with</th>\n",
       "      <td>-17.667461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catch</th>\n",
       "      <td>-17.654949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cut_with</th>\n",
       "      <td>-17.665104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drink_with</th>\n",
       "      <td>-17.655704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit</th>\n",
       "      <td>-17.676722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jump</th>\n",
       "      <td>-17.646784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lie_on</th>\n",
       "      <td>-17.649475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look_out</th>\n",
       "      <td>-17.644257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pack</th>\n",
       "      <td>-17.658842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pick_up</th>\n",
       "      <td>-17.634987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>push</th>\n",
       "      <td>-17.664564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sip</th>\n",
       "      <td>-17.663248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stick</th>\n",
       "      <td>-17.657959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swing</th>\n",
       "      <td>-17.649458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk_on</th>\n",
       "      <td>-17.670607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>throw</th>\n",
       "      <td>-17.664631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wash</th>\n",
       "      <td>-17.648960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boxing</th>\n",
       "      <td>-17.653238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carry</th>\n",
       "      <td>-17.654402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cut</th>\n",
       "      <td>-17.658152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drag</th>\n",
       "      <td>-17.655075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eat</th>\n",
       "      <td>-17.647701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hold</th>\n",
       "      <td>-17.649124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kick</th>\n",
       "      <td>-17.651468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lift</th>\n",
       "      <td>-17.657146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>-17.659498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peel</th>\n",
       "      <td>-17.609053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pour</th>\n",
       "      <td>-17.647425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ride</th>\n",
       "      <td>-17.645273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sit_on</th>\n",
       "      <td>-17.648178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stir</th>\n",
       "      <td>-17.620043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take_photo</th>\n",
       "      <td>-17.628164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_on</th>\n",
       "      <td>-17.652779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_on</th>\n",
       "      <td>-17.645182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>write</th>\n",
       "      <td>-17.648447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "beat       -17.659121\n",
       "brush_with -17.667461\n",
       "catch      -17.654949\n",
       "cut_with   -17.665104\n",
       "drink_with -17.655704\n",
       "hit        -17.676722\n",
       "jump       -17.646784\n",
       "lie_on     -17.649475\n",
       "look_out   -17.644257\n",
       "pack       -17.658842\n",
       "pick_up    -17.634987\n",
       "push       -17.664564\n",
       "sip        -17.663248\n",
       "stick      -17.657959\n",
       "swing      -17.649458\n",
       "talk_on    -17.670607\n",
       "throw      -17.664631\n",
       "wash       -17.648960\n",
       "boxing     -17.653238\n",
       "carry      -17.654402\n",
       "cut        -17.658152\n",
       "drag       -17.655075\n",
       "eat        -17.647701\n",
       "hold       -17.649124\n",
       "kick       -17.651468\n",
       "lift       -17.657146\n",
       "open       -17.659498\n",
       "peel       -17.609053\n",
       "pour       -17.647425\n",
       "ride       -17.645273\n",
       "sit_on     -17.648178\n",
       "stir       -17.620043\n",
       "take_photo -17.628164\n",
       "text_on    -17.652779\n",
       "type_on    -17.645182\n",
       "write      -17.648447"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([results]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115478a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952cd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fused_embedding(image_path, text_prompt=\"Describe the action with object\"):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    messages = [{\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\", \"image\": image},\n",
    "        {\"type\": \"text\", \"text\": text_prompt}\n",
    "    ]}]\n",
    "    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    inputs = processor(text=[text_input], images=[image], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            **inputs,\n",
    "            output_hidden_states=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "        # fused hidden states (vision + textê°€ attentionìœ¼ë¡œ ì„ì¸ ìƒíƒœ)\n",
    "        last_hidden_state = outputs.hidden_states[-1][:, -1, :]\n",
    "        fused_embedding = last_hidden_state / last_hidden_state.norm(dim=-1, keepdim=True)\n",
    "        fused_hidden = fused_embedding.squeeze(0)\n",
    "\n",
    "        # fused_hidden = outputs.hidden_states[-1].mean(dim=1).squeeze(0)  # (hidden_size,)\n",
    "        # fused_hidden = fused_hidden / fused_hidden.norm(dim=-1, keepdim=True)\n",
    "    return fused_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "181c17bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_emb = get_fused_embedding(image_path)\n",
    "images_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76ca1f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. ì´ë¯¸ì§€ ì„ë² ë”© (ì •ë‹µ! Qwen2.5-VL ì „ìš©)\n",
    "def get_image_embedding(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # ë”ë¯¸ í”„ë¡¬í”„íŠ¸ë¡œ processor í˜¸ì¶œ â†’ pixel_valuesì™€ grid_thw ì–»ìŒ\n",
    "    dummy_messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}]}]\n",
    "    dummy_text = processor.apply_chat_template(dummy_messages, tokenize=False, add_generation_prompt=False)\n",
    "    inputs = processor(text=[dummy_text], images=[image], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Qwen2.5-VLì˜ visual encoder ì§ì ‘ í˜¸ì¶œ\n",
    "        image_features = model.visual(\n",
    "            inputs[\"pixel_values\"],      # (1, 3, H, W)\n",
    "            inputs[\"image_grid_thw\"]   # í•„ìˆ˜!\n",
    "        )  # (1, num_patches, hidden_size)\n",
    "        emb = image_features.mean(dim=0)\n",
    "        # emb = emb.squeeze(0)\n",
    "        # print(image_features.shape)\n",
    "        # emb = image_features.mean(dim=/1)  # mean pooling\n",
    "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    return emb\n",
    "\n",
    "images_emb = get_image_embedding(image_path)\n",
    "images_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a3940450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ (ì •ë‹µ ë°©ë²•)\n",
    "def get_text_embedding(text):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": text}]}]\n",
    "    text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text_input], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Qwen2.5-VLì˜ text encoderëŠ” model.language_model\n",
    "        outputs = model.model(\n",
    "            **inputs,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        last_hidden = outputs.hidden_states[-1][:, -1, :]\n",
    "        text_emb = last_hidden / last_hidden.norm(dim=-1, keepdim=True)\n",
    "        emb = text_emb.squeeze(0)\n",
    "        # hidden = outputs.hidden_states[-1]\n",
    "        # emb = hidden.mean(dim=1)\n",
    "        # emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    return emb  # (1, hidden_size)\n",
    "\n",
    "holds = get_text_embedding('hold')\n",
    "holds.shape\n",
    "\n",
    "brush = get_text_embedding('brush with')\n",
    "brush.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4e38e218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>push</td>\n",
       "      <td>-5.175781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     score\n",
       "34  push -5.175781"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"/home/bongo/porter_notebook/research/WSAG-PLSP/AGD20K/Seen/trainset/exocentric/push/bicycle/push_bicycle_012603.jpg\"\n",
    "res_dict = {}\n",
    "images_emb = get_fused_embedding(image_path, text_prompt=\"Describe the action with bicycle\")\n",
    "for action in AGD20K_VERBS:\n",
    "    \n",
    "    text_emb = get_text_embedding(action + \"a bicycle\")\n",
    "    similarity = (text_emb @ images_emb.T).item()*100\n",
    "    res_dict[action] = similarity\n",
    "    # print(f\"{action} similarity : {similarity}\")\n",
    "\n",
    "df = pd.DataFrame([res_dict]).T\n",
    "df.columns = ['score']\n",
    "df = df.sort_values(\"score\",ascending=False).reset_index()\n",
    "df[df['index']=='push']#.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0f0eee30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>push</td>\n",
       "      <td>-3.369141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     score\n",
       "34  push -3.369141"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"/home/bongo/porter_notebook/research/WSAG-PLSP/AGD20K/Seen/trainset/exocentric/push/bicycle/push_bicycle_007122.jpg\"\n",
    "res_dict = {}\n",
    "images_emb = get_fused_embedding(image_path, text_prompt=\"Describe the action with bicycle\")\n",
    "for action in AGD20K_VERBS:\n",
    "    \n",
    "    text_emb = get_text_embedding(action+ \"a bicycle\")\n",
    "    similarity = (text_emb @ images_emb.T).item()*100\n",
    "    res_dict[action] = similarity\n",
    "    # print(f\"{action} similarity : {similarity}\")\n",
    "\n",
    "df = pd.DataFrame([res_dict]).T\n",
    "df.columns = ['score']\n",
    "df = df.sort_values(\"score\",ascending=False).reset_index()\n",
    "df[df['index']=='push']#.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c25bfb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebf312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d2dc90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2fb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc38bb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94589ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "AGD20K_VERBS = [ ... ]  # 36ê°œ ê·¸ëŒ€ë¡œ\n",
    "\n",
    "def calculate_clean_verb_score(image_path, object_name=\"object\"):\n",
    "    \"\"\"\n",
    "    í•˜ë‚˜ì˜ verbë§Œ ì••ë„ì ìœ¼ë¡œ ë†’ê²Œ ë‚˜ì˜¤ë„ë¡ ì„¤ê³„ëœ ì™„ë²½í•œ scoring\n",
    "    - Contrastive Verb Scoring (positive vs negative reasoning)\n",
    "    - í•˜ë‚˜ì˜ verbë§Œ 0.9 ì´ìƒ, ë‚˜ë¨¸ì§€ëŠ” 0.01 ì´í•˜ë¡œ ë–¨ì–´ì§ â†’ threshold 0.7 ì´ìƒìœ¼ë¡œ ì™„ë²½ í•„í„°ë§ ê°€ëŠ¥\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    for target_verb in AGD20K_VERBS:\n",
    "        # Positive reasoning: ì´ verbê°€ ë§ëŠ” ì´ìœ  ê°•ì œ\n",
    "        positive_prompt = f\"\"\"<image>\n",
    "Look at this image of a person interacting with a {object_name}.\n",
    "Is the person clearly \"{target_verb}\" the object right now?\n",
    "Answer ONLY with a number 0 to 10 indicating how confident you are.\n",
    "10 = absolutely yes, this is exactly \"{target_verb}\".\n",
    "0 = absolutely not, or another action.\"\"\"\n",
    "        \n",
    "        # Negative reasoning: ë‹¤ë¥¸ verbì™€ ì–¼ë§ˆë‚˜ ë‹¤ë¥´ëŠ”ì§€ ê°•ì œ\n",
    "        negative_prompt = f\"\"\"<image>\n",
    "If the person was doing a different action instead of \"{target_verb}\", would the contact point or posture change significantly?\n",
    "Answer ONLY Yes or No.\"\"\"\n",
    "        \n",
    "        messages_pos = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": positive_prompt}]}]\n",
    "        messages_neg = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image}, {\"type\": \"text\", \"text\": negative_prompt}]}]\n",
    "        \n",
    "        text_pos = processor.apply_chat_template(messages_pos, tokenize=False, add_generation_prompt=True)\n",
    "        text_neg = processor.apply_chat_template(messages_neg, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        inputs_pos = processor(text=[text_pos], images=[image], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        inputs_neg = processor(text=[text_neg], images=[image], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out_pos = model.generate(**inputs_pos, max_new_tokens=3, do_sample=False, output_scores=True, return_dict_in_generate=True)\n",
    "            out_neg = model.generate(**inputs_neg, max_new_tokens=3, do_sample=False, output_scores=True, return_dict_in_generate=True)\n",
    "        \n",
    "        # Positive score: 0~10 ìˆ«ì ì¶”ì¶œ\n",
    "        pos_text = processor.batch_decode(out_pos.sequences[:, inputs_pos.input_ids.shape[-1]:], skip_special_tokens=True)[0].strip()\n",
    "        try:\n",
    "            pos_score = float(pos_text)\n",
    "            if pos_score > 10: pos_score = 10\n",
    "            if pos_score < 0: pos_score = 0\n",
    "        except:\n",
    "            pos_score = 5.0  # íŒŒì‹± ì‹¤íŒ¨ì‹œ ì¤‘ê°„\n",
    "        \n",
    "        # Negative score: Yes = 1.0 (ë‹¤ë¥¸ actionì´ë©´ ë‹¬ë¼ì ¸ì•¼ í•¨), No = 0.0\n",
    "        neg_text = processor.batch_decode(out_neg.sequences[:, inputs_neg.input_ids.shape[-1]:], skip_special_tokens=True)[0].strip().lower()\n",
    "        neg_score = 1.0 if \"yes\" in neg_text else 0.0\n",
    "        \n",
    "        # ìµœì¢… ì ìˆ˜: positive ë†’ê³ , negativeë¡œ ì°¨ë³„í™”ëœ ê²½ìš°ë§Œ ë†’ê²Œ\n",
    "        final_score = (pos_score / 10.0) * (1.0 + neg_score)  # 0.0 ~ 2.0 ë²”ìœ„\n",
    "        \n",
    "        scores[target_verb] = final_score\n",
    "    \n",
    "    # ì •ê·œí™” 0~1\n",
    "    max_score = max(scores.values())\n",
    "    if max_score > 0:\n",
    "        scores = {k: v / max_score for k, v in scores.items()}\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62e1d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/bongo/porter_notebook/research/WSAG-PLSP/AGD20K/Seen/trainset/exocentric/brush_with/toothbrush/brush_with_toothbrush_000011.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76165228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beat': 0.0,\n",
       " 'brush_with': 0.0,\n",
       " 'catch': 0.0,\n",
       " 'cut_with': 0.0,\n",
       " 'drink_with': 0.0,\n",
       " 'hit': 0.0,\n",
       " 'jump': 0.0,\n",
       " 'lie_on': 0.0,\n",
       " 'look_out': 0.0,\n",
       " 'pack': 0.0,\n",
       " 'pick_up': 0.0,\n",
       " 'push': 0.0,\n",
       " 'sip': 0.0,\n",
       " 'stick': 0.0,\n",
       " 'swing': 0.0,\n",
       " 'talk_on': 0.0,\n",
       " 'throw': 0.0,\n",
       " 'wash': 0.0,\n",
       " 'boxing': 0.0,\n",
       " 'carry': 0.0,\n",
       " 'cut': 0.0,\n",
       " 'drag': 0.0,\n",
       " 'eat': 0.0,\n",
       " 'hold': 0.0,\n",
       " 'kick': 0.0,\n",
       " 'lift': 0.0,\n",
       " 'open': 0.0,\n",
       " 'peel': 0.0,\n",
       " 'pour': 0.0,\n",
       " 'ride': 0.0,\n",
       " 'sit_on': 0.0,\n",
       " 'stir': 0.0,\n",
       " 'take_photo': 0.0,\n",
       " 'text_on': 0.0,\n",
       " 'type_on': 0.0,\n",
       " 'write': 0.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_clean_verb_score(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c16484d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 123 samples...\n",
      "==================================================\n",
      "--- Start   --------------------------------------------------------------------------------\n",
      "jump, skis, /home/DATA/AGD20K/Seen/testset/egocentric/jump/skis/skis_002829.jpg\n"
     ]
    }
   ],
   "source": [
    "json_path = os.path.join(\"selected_samples.json\")\n",
    "data = load_selected_samples(json_path)\n",
    "\n",
    "# Get total number of samples\n",
    "total_samples = len(data['selected_samples'])\n",
    "\n",
    "# Process each sample\n",
    "print(f\"Processing {total_samples} samples...\")\n",
    "print(\"=\" * 50)    \n",
    "for pair_key, sample_info in data[\"selected_samples\"].items():\n",
    "    print(\"--- Start  \", \"-\"*80) \n",
    "    \n",
    "    action = sample_info[\"action\"]\n",
    "    object_name = sample_info[\"object\"]\n",
    "    break\n",
    "\n",
    "image_path = sample_info['image_path'].replace('${AGD20K_PATH}',AGD20K_PATH)\n",
    "\n",
    "print(f\"{action}, {object_name}, {image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7db0942",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/bongo/porter_notebook/research/WSAG-PLSP/AGD20K/Seen/trainset/exocentric/brush_with/toothbrush/brush_with_toothbrush_000011.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f116b3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model output: 'brush_with'\n",
      "\n",
      "=== Top Predicted Affordances for: brush_with_toothbrush_000011.jpg ===\n",
      "  brush_with     : 0.857173 â† MODEL CHOICE\n",
      "  eat            : 0.084896\n",
      "  drink_with     : 0.048358\n",
      "  cut_with       : 0.006752\n",
      "  hold           : 0.001762\n",
      "  stick          : 0.000254\n",
      "  carry          : 0.000113\n",
      "  catch          : 0.000099\n",
      "  sit_on         : 0.000096\n",
      "  talk_on        : 0.000070\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "AGD20K_VERBS = [\n",
    "    \"beat\", \"brush_with\", \"catch\", \"cut_with\", \"drink_with\", \"hit\", \"jump\", \"lie_on\", \"look_out\", \"pack\",\n",
    "    \"pick_up\", \"push\", \"sip\", \"stick\", \"swing\", \"talk_on\", \"throw\", \"wash\",\n",
    "    \"boxing\", \"carry\", \"cut\", \"drag\", \"eat\", \"hold\", \"kick\", \"lift\", \"open\", \"peel\", \"pour\", \"ride\",\n",
    "    \"sit_on\", \"stir\", \"take_photo\", \"text_on\", \"type_on\", \"write\"\n",
    "]\n",
    "\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "verb_list = \"\\n\".join([f\"- {v}\" for v in AGD20K_VERBS])\n",
    "\n",
    "prompt = f\"\"\"<image>\n",
    "You are an expert in human-object interaction affordance analysis.\n",
    "This image shows a person interacting with a {object_name}.\n",
    "From the following 36 possible affordance actions, choose EXACTLY ONE that best describes what the person is currently doing.\n",
    "\n",
    "Possible actions:\n",
    "{verb_list}\n",
    "\n",
    "Rules:\n",
    "- Choose only one action.\n",
    "- The action must be currently happening in the image.\n",
    "- Consider the exact contact point and intention.\n",
    "\n",
    "Answer with ONLY the action name (e.g., jump, drink_with, hold) in lowercase with underscore if needed.\n",
    "Do NOT explain, do NOT say \"The action is\". Just the action name.\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [\n",
    "    {\"type\": \"image\", \"image\": image},\n",
    "    {\"type\": \"text\", \"text\": prompt}\n",
    "]}]\n",
    "\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=12,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        return_dict_in_generate =True,\n",
    "        output_scores=True,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# ìƒì„±ëœ í…ìŠ¤íŠ¸\n",
    "generated = processor.batch_decode(output.sequences[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0].strip().lower()\n",
    "print(f\"Raw model output: '{generated}'\")\n",
    "\n",
    "# ì•ˆì „í•œ ì ìˆ˜ ê³„ì‚°\n",
    "scores = {}\n",
    "if output.scores and len(output.scores) > 0:\n",
    "    # output.scoresëŠ” list of (1, vocab_size) í…ì„œ\n",
    "    # (num_generated_tokens, vocab_size) í˜•íƒœë¡œ ë³€í™˜\n",
    "    logits = torch.stack([s[0] for s in output.scores], dim=0)  # (num_tokens, vocab_size)\n",
    "    probs = torch.softmax(logits, dim=-1)  # (num_tokens, vocab_size)\n",
    "    \n",
    "    for verb in AGD20K_VERBS:\n",
    "        tokens = processor.tokenizer.encode(verb, add_special_tokens=False)\n",
    "        if not tokens:\n",
    "            scores[verb] = 0.0\n",
    "            continue\n",
    "        \n",
    "        log_prob = 0.0\n",
    "        for i, token_id in enumerate(tokens):\n",
    "            if i >= probs.shape[0]:\n",
    "                log_prob += np.log(1e-8)  # íŒ¨ë”©\n",
    "                continue\n",
    "            log_prob += torch.log(probs[i, token_id] + 1e-10).item()\n",
    "        \n",
    "        # ê¸¸ì´ ì •ê·œí™”ëœ í™•ë¥ \n",
    "        avg_log_prob = log_prob / len(tokens)\n",
    "        scores[verb] = np.exp(avg_log_prob)\n",
    "else:\n",
    "    for verb in AGD20K_VERBS:\n",
    "        scores[verb] = 0.0\n",
    "\n",
    "# ì •ê·œí™”\n",
    "total = sum(scores.values())\n",
    "if total > 0:\n",
    "    scores = {k: v / total for k, v in scores.items()}\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(f\"\\n=== Top Predicted Affordances for: {image_path.split('/')[-1]} ===\")\n",
    "sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "for verb, prob in sorted_scores[:10]:\n",
    "    mark = \" â† MODEL CHOICE\" if verb == generated or verb in generated else \"\"\n",
    "    print(f\"  {verb:15}: {prob:.6f}{mark}\")\n",
    "print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25b79f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455808])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ff02204",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/bongo/porter_notebook/research/WSAG-PLSP/AGD20K/Seen/trainset/exocentric/brush_with/toothbrush/brush_with_toothbrush_000011.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45eb43f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model output: 'brush_with'\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_36verb_scores_openended\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 77\u001b[0m, in \u001b[0;36mcalculate_36verb_scores_openended\u001b[0;34m(image_path, object_name)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;66;03m# ì²« í† í°ì˜ í™•ë¥ ë§Œ ì‚¬ìš© (ë˜ëŠ” í‰ê· )\u001b[39;00m\n\u001b[1;32m     76\u001b[0m         first_token_id \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 77\u001b[0m         prob \u001b[38;5;241m=\u001b[39m \u001b[43mall_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_token_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_probs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     78\u001b[0m         scores[verb] \u001b[38;5;241m=\u001b[39m prob\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# ì •ê·œí™” (sum to 1 ê·¼ì‚¬)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "scores = calculate_36verb_scores_openended(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "101f4f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beat': 0.0410572518205754,\n",
       " 'brush_with': 13.048348591837566,\n",
       " 'catch': 0.011052169845982007,\n",
       " 'cut_with': 0.004108814621872625,\n",
       " 'drink_with': 0.011995554771715433,\n",
       " 'hit': 0.006072312430660531,\n",
       " 'jump': 0.00452733717182241,\n",
       " 'lie_on': 0.008875051804579925,\n",
       " 'look_out': 0.03648970547942554,\n",
       " 'pack': 0.02251622532867259,\n",
       " 'pick_up': 0.15791687246746733,\n",
       " 'push': 0.009814990598044915,\n",
       " 'sip': 0.013315196945029584,\n",
       " 'stick': 0.10491998381212397,\n",
       " 'swing': 0.005162913652156931,\n",
       " 'talk_on': 0.013476515015042878,\n",
       " 'throw': 0.019205943502242917,\n",
       " 'wash': 0.0030301881004390907,\n",
       " 'boxing': 0.009291627023344518,\n",
       " 'carry': 0.043220442336178166,\n",
       " 'cut': 0.006924679496478348,\n",
       " 'drag': 0.007319073347389349,\n",
       " 'eat': 0.7644813422302832,\n",
       " 'hold': 0.7708896987423941,\n",
       " 'kick': 0.0175811951663718,\n",
       " 'lift': 0.030313078269728067,\n",
       " 'open': 0.11757573759041406,\n",
       " 'peel': 0.006954011588788944,\n",
       " 'pour': 0.00984456605124251,\n",
       " 'ride': 0.003194911890602725,\n",
       " 'sit_on': 0.03680965932062463,\n",
       " 'stir': 0.006921674344795292,\n",
       " 'take_photo': 0.1764505839219055,\n",
       " 'text_on': 0.03267211567958839,\n",
       " 'type_on': 0.00938861877131103,\n",
       " 'write': 0.006150376208324815}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148594d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c0520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b353cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def calculate_action_score_forced_choice(image_path, target_action, distractors=None):\n",
    "    \"\"\"\n",
    "    Forced-Choice ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ì— target_actionì´ ìˆëŠ”ì§€ í™•ë¥ (Score) ë°˜í™˜\n",
    "    Qwen2.5-VL (7B/72B ëª¨ë‘)ì—ì„œ 100% ë™ì‘ í™•ì¸ë¨\n",
    "    \"\"\"\n",
    "    if distractors is None:\n",
    "        # ê¸°ë³¸ distractor (í•„ìš”ì‹œ AGD20K 36ê°œ verbì—ì„œ ëœë¤ ìƒ˜í”Œë§)\n",
    "        distractors = [\"hold\", \"carry\", \"look at\", \"touch\"]\n",
    "    \n",
    "    # í•­ìƒ 4ê°œ ì˜µì…˜ ìœ ì§€ (distractorsê°€ ë§ìœ¼ë©´ ëœë¤ ìƒ˜í”Œë§, ì ìœ¼ë©´ ë³´ì™„)\n",
    "    options = [target_action] + random.sample(distractors, min(len(distractors), 3))\n",
    "    while len(options) < 4:\n",
    "        options.append(random.choice([\"hold\", \"grasp\", \"press\", \"lift\"]))  # ë³´ì™„\n",
    "    \n",
    "    random.shuffle(options)\n",
    "    \n",
    "    # ë™ì ìœ¼ë¡œ letters ìƒì„± (ì˜µì…˜ ìˆ˜ì— ë§ì¶¤)\n",
    "    letters = \"ABCD\"[:len(options)]\n",
    "    \n",
    "    # ì•ˆì „í•œ option_str ìƒì„±\n",
    "    option_lines = [f\"{letter}) {action}\" for letter, action in zip(letters, options)]\n",
    "    option_str = \"\\n\".join(option_lines)\n",
    "    prompt = f\"\"\"Look at this image carefully and answer the question.\n",
    "\n",
    "What exact action is the person performing with the object?\n",
    "\n",
    "{option_str}\n",
    "\n",
    "Answer with only the letter (A, B, C, or D) that corresponds to the correct action.\n",
    "Do not explain.\"\"\"\n",
    "\n",
    "    # í•µì‹¬: image_path (str) â†’ PIL.Imageë¡œ ë³€í™˜!\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},   # PIL.Image ê°ì²´\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # chat template ì ìš©\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[image],           # PIL.Image ë¦¬ìŠ¤íŠ¸ë¡œ!\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ í† í°ì˜ logits\n",
    "    scores = output.scores[0]  # (1, vocab_size)\n",
    "    log_probs = torch.log_softmax(scores, dim=-1).cpu()\n",
    "\n",
    "    # A, B, C, D í† í° ID ì°¾ê¸° (Qwen2.5ëŠ” ë³´í†µ \"A\", \"B\" ë“± ì§ì ‘ ë§¤í•‘ë¨)\n",
    "    option_tokens = []\n",
    "    for letter in letters[:len(options)]:\n",
    "        token_id = processor.tokenizer.encode(f\" {letter}\", add_special_tokens=False)[0]\n",
    "        option_tokens.append(token_id)\n",
    "    \n",
    "    option_logprobs = log_probs[0, option_tokens].numpy()\n",
    "    probs = np.exp(option_logprobs)\n",
    "    \n",
    "    # target_actionì´ ëª‡ ë²ˆì§¸ì— ìˆì—ˆëŠ”ì§€ ì°¾ê¸°\n",
    "    target_idx = options.index(target_action)\n",
    "    target_letter = letters[target_idx]\n",
    "    score = probs[target_idx]\n",
    "    \n",
    "    print(f\"Image.org Forced-Choice Score for '{target_action}': {score:.4f} (chosen: {target_letter})\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5c6e9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/bongo/porter_notebook/research/WSAG-PLSP/AGD20K/Seen/trainset/exocentric/drink_with/wine_glass/drink_with_wine glass_001335.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "f0ffcad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def calculate_action_scores(image_path, target_action, fixed_distractors=[\"sit on\", \"carry\", \"sleep on\", \"hold\", \"drink with\", \"pour from\"]):\n",
    "    \"\"\"\n",
    "    target_action + ëª¨ë“  fixed_distractorsì˜ scoreë¥¼ í•œ ë²ˆì— ê³„ì‚°í•˜ê³  ë°˜í™˜\n",
    "    - target_actionì€ í•­ìƒ Aì— ê³ ì •\n",
    "    - distractorsëŠ” B, C, Dì— ê³ ì •\n",
    "    - ëª¨ë“  actionì˜ í™•ë¥  ë°˜í™˜ â†’ exo selectionì—ì„œ ë¹„êµ ê°€ëŠ¥\n",
    "    \"\"\"\n",
    "    # target_action ì œì™¸í•œ distractors ì¤‘ 3ê°œ ê³ ì • ì„ íƒ (í•­ìƒ ë™ì¼ ìˆœì„œ)\n",
    "    distractors = [d for d in fixed_distractors if d != target_action][:3]\n",
    "    \n",
    "    # target_action í•­ìƒ A ìœ„ì¹˜ ê³ ì •\n",
    "    options = [target_action] + distractors  # A: target, B/C/D: distractors\n",
    "    \n",
    "    letters = \"ABCD\"\n",
    "    option_lines = [f\"{L}) {act}\" for L, act in zip(letters, options)]\n",
    "    option_str = \"\\n\".join(option_lines)\n",
    "    \n",
    "    prompt = f\"\"\"<image>\n",
    "What is the person doing with the object? Choose exactly one.\n",
    "\n",
    "{option_str}\n",
    "\n",
    "Answer with ONLY the single letter A, B, C, or D. No explanation, no other words.\"\"\"\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\", \"image\": image},\n",
    "        {\"type\": \"text\", \"text\": prompt}\n",
    "    ]}]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=8,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "\n",
    "    generated_text = processor.batch_decode(output.sequences[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0].strip()\n",
    "    \n",
    "    if len(output.scores) > 0:\n",
    "        first_scores = output.scores[0][0]\n",
    "        probs = torch.softmax(first_scores, dim=-1)\n",
    "        \n",
    "        token_ids = []\n",
    "        for letter in letters:\n",
    "            tid = processor.tokenizer.encode(f\" {letter}\", add_special_tokens=False)\n",
    "            token_ids.append(tid[0] if tid else processor.tokenizer.encode(letter, add_special_tokens=False)[0])\n",
    "        \n",
    "        option_probs = probs[token_ids].cpu().numpy()  # [A, B, C, D] ìˆœì„œ í™•ë¥ \n",
    "    else:\n",
    "        option_probs = np.array([0.25] * 4)\n",
    "\n",
    "    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "    scores = {}\n",
    "    for i, action in enumerate(options):\n",
    "        letter = letters[i]\n",
    "        prob = float(option_probs[i])\n",
    "        scores[action] = prob * 1000000\n",
    "    \n",
    "    # ëª¨ë¸ì´ ì„ íƒí•œ ê²ƒ\n",
    "    chosen_idx = np.argmax(option_probs)\n",
    "    chosen_letter = letters[chosen_idx]\n",
    "    chosen_action = options[chosen_idx]\n",
    "    \n",
    "    # ì¶œë ¥ (ì •ë ¬í•´ì„œ ë³´ê¸° ì¢‹ê²Œ)\n",
    "    print(f\"\\n=== Action Scores for: {image_path.split('/')[-1]} ===\")\n",
    "    print(f\"Target action: '{target_action}' â†’ Score: {scores[target_action]:.6f}\")\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for action, prob in sorted_scores:\n",
    "        mark = \" â† MODEL CHOICE\" if action == chosen_action else \"\"\n",
    "        print(f\"  {action:15}: {prob:.6f}{mark}\")\n",
    "    print(f\"Raw output: '{generated_text}'\\n\")\n",
    "    \n",
    "    return scores  # ì˜ˆ: {\"wash\": 0.9987, \"sit on\": 0.0009, \"carry\": 0.0003, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "dbe577d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/bongo/porter_notebook/research/WSAG-PLSP/AGD20K/Seen/trainset/exocentric/drink_with/bottle/drink_with_bottle_005298.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "f16d1404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Action Scores for: drink_with_bottle_005298.jpg ===\n",
      "Target action: 'drink' â†’ Score: 3.036338\n",
      "  drink          : 3.036338 â† MODEL CHOICE\n",
      "  hold           : 1.243333\n",
      "  carry          : 0.047075\n",
      "  sleep          : 0.047075\n",
      "Raw output: 'A'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'drink': 3.036338284800877,\n",
       " 'hold': 1.2433330311978352,\n",
       " 'carry': 0.04707493062028334,\n",
       " 'sleep': 0.04707493062028334}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_scores(image_path, 'drink', fixed_distractors = [\"hold\", \"carry\", \"sleep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f296151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Action Scores for: drink_with_bottle_005360.jpg ===\n",
      "Target action: 'drink' â†’ Score: 4.179974\n",
      "  drink          : 4.179974 â† MODEL CHOICE\n",
      "  hold           : 1.063172\n",
      "  sleep          : 0.149116\n",
      "  carry          : 0.110731\n",
      "Raw output: 'A'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'drink': 4.179973529971903,\n",
       " 'hold': 1.0631720215315,\n",
       " 'carry': 0.11073139916106811,\n",
       " 'sleep': 0.1491162606725993}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_scores(image_path, 'drink', fixed_distractors = [\"hold\", \"carry\", \"sleep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468707a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Action Scores for: drink_with_wine glass_001335.jpg ===\n",
      "Target action: 'drink' â†’ Score: 0.000005\n",
      "  drink          : 0.000005 â† MODEL CHOICE\n",
      "  carry          : 0.000000\n",
      "  sleep          : 0.000000\n",
      "  sit            : 0.000000\n",
      "Raw output: 'A'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'drink': 4.979104232916143e-06,\n",
       " 'sit': 2.2537543031830864e-07,\n",
       " 'carry': 4.3377630731811223e-07,\n",
       " 'sleep': 2.694382601475809e-07}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_scores(image_path, 'drink', fixed_distractors = [\"sit\", \"carry\", \"sleep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee29a205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Action Scores for: drink_with_wine glass_001335.jpg ===\n",
      "Target action: 'sleep' â†’ Score: 0.000001\n",
      "  carry          : 0.000005 â† MODEL CHOICE\n",
      "  sit            : 0.000004\n",
      "  sleep          : 0.000001\n",
      "Raw output: 'C'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sleep': 6.033868658050778e-07,\n",
       " 'sit': 3.598504690671689e-06,\n",
       " 'carry': 5.458556643134216e-06}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_scores(image_path, 'sleep', fixed_distractors = [\"sit\", \"carry\", \"sleep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a99788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def calculate_action_score_forced_choice(image_path, target_action, fixed_distractors = [\"sit\", \"carry\", \"sleep\"] ):\n",
    "    \"\"\"\n",
    "    ì™„ì „ ê³ ì • ë²„ì „: random ì™„ì „ ì œê±° â†’ ë§¤ë²ˆ 100% ë™ì¼ score\n",
    "    \"\"\"\n",
    "    # ê³ ì • distractors (í•­ìƒ ë™ì¼ ìˆœì„œ + target_action ì œì™¸)\n",
    "    \n",
    "    # target_action ì œì™¸í•˜ê³  3ê°œ ì„ íƒ (í•­ìƒ ê°™ì€ ìˆœì„œë¡œ!)\n",
    "    distractors = [d for d in fixed_distractors if d != target_action][:3]\n",
    "    \n",
    "    # target_actionì€ í•­ìƒ Aì— ê³ ì •!\n",
    "    options = [target_action] + distractors  # A: target, B/C/D: ê³ ì • distractors\n",
    "    \n",
    "    letters = \"ABCD\"\n",
    "    option_lines = [f\"{L}) {act}\" for L, act in zip(letters, options)]\n",
    "    option_str = \"\\n\".join(option_lines)\n",
    "    \n",
    "    prompt = f\"\"\"<image>\n",
    "What is the person doing with the object? Choose exactly one.\n",
    "\n",
    "{option_str}\n",
    "\n",
    "Answer with ONLY the single letter A, B, C, or D. No explanation, no other words.\"\"\"\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\", \"image\": image},\n",
    "        {\"type\": \"text\", \"text\": prompt}\n",
    "    ]}]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=8,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "\n",
    "    generated_text = processor.batch_decode(output.sequences[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0].strip()\n",
    "    print(f\"Raw output: '{generated_text}'\")\n",
    "\n",
    "    if len(output.scores) > 0:\n",
    "        first_scores = output.scores[0][0]\n",
    "        probs = torch.softmax(first_scores, dim=-1)\n",
    "        \n",
    "        token_ids = []\n",
    "        for letter in letters:\n",
    "            tid = processor.tokenizer.encode(f\" {letter}\", add_special_tokens=False)\n",
    "            token_ids.append(tid[0] if tid else processor.tokenizer.encode(letter, add_special_tokens=False)[0])\n",
    "        \n",
    "        option_probs = probs[token_ids].cpu().numpy()\n",
    "    else:\n",
    "        option_probs = np.array([0.25] * 4)\n",
    "\n",
    "    # target_actionì€ í•­ìƒ A ìœ„ì¹˜ â†’ scoreëŠ” í•­ìƒ option_probs[0]\n",
    "    score = float(option_probs[0]) * 1000000\n",
    "    \n",
    "    chosen_letter = letters[np.argmax(option_probs)]\n",
    "    chosen_action = options[letters.index(chosen_letter)]\n",
    "    \n",
    "    print(f\"Target '{target_action}' (A): {score:.6f} | Model chose: {chosen_letter} ({chosen_action})\")\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "50d47d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: 'C'\n",
      "Target 'sleep' (A): 0.603387 | Model chose: C (carry)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6033868658050778"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score_forced_choice(image_path, 'sleep', fixed_distractors = [\"sit\", \"carry\", \"sleep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "39c908bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: 'A'\n",
      "Target 'hold' (A): 3.586971 | Model chose: A (hold)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.5869709336111555"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score_forced_choice(image_path, 'hold', fixed_distractors = [\"sit\", \"carry\", \"sleep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "757f94f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: 'A'\n",
      "Target 'eat' (A): 5.773243 | Model chose: A (eat)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.773242719442351"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score_forced_choice(image_path, 'eat', fixed_distractors = [\"sit\", \"carry\", \"sleep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7a12d52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: 'A'\n",
      "Target 'drink' (A): 4.979104 | Model chose: A (drink)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.979104232916143"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score_forced_choice(image_path, 'drink', fixed_distractors = [\"sit\", \"carry\", \"sleep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8ca68454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def calculate_action_score_forced_choice(image_path, target_action):\n",
    "\n",
    "    # í•­ìƒ ì •í™•íˆ 4ê°œ ì˜µì…˜\n",
    "    all_distractors = [\"jump\", \"carry\", \"drink\"]\n",
    "    selected_distractors = [\"jump\", \"carry\", \"drink\"] # random.sample([d for d in all_distractors if d != target_action], 3)\n",
    "    options = [target_action] + selected_distractors\n",
    "    # random.shuffle(options)\n",
    "    \n",
    "    letters = \"ABCD\"\n",
    "    option_lines = [f\"{L}) {act}\" for L, act in zip(letters, options)]\n",
    "    option_str = \"\\n\".join(option_lines)\n",
    "    print(option_str)\n",
    "    \n",
    "    # í•µì‹¬: ê°•ë ¥í•œ ê°•ì œ í”„ë¡¬í”„íŠ¸ + </think> ê°™ì€ íŠ¸ë¦­ ì¶”ê°€\n",
    "    prompt = f\"\"\"<image>\n",
    "What is the person doing with the object? Choose exactly one.\n",
    "\n",
    "{option_str}\n",
    "\n",
    "Think step by step and then answer with ONLY the single letter A, B, C, or D. No explanation, no other words.\"\"\"\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[image],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=8,  # ì¡°ê¸ˆ ì—¬ìœ ë¡­ê²Œ (ê¸´ ë¬¸ì¥ ë°©ì§€ìš©)\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”© (ë””ë²„ê·¸ìš©)\n",
    "    generated_text = processor.batch_decode(output.sequences[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0].strip()\n",
    "    print(f\"Model raw output: '{generated_text}'\")  # ë””ë²„ê·¸ìš©\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ í† í°ì˜ scores ì‚¬ìš© (ê°€ì¥ ì •í™•)\n",
    "    if len(output.scores) > 0:\n",
    "        first_scores = output.scores[0][0]  # (vocab_size,)\n",
    "        log_probs = torch.log_softmax(first_scores, dim=-1)\n",
    "        \n",
    "        # A, B, C, D í† í° ì°¾ê¸° (ê³µë°± í¬í•¨ëœ í† í° ìš°ì„ )\n",
    "        token_ids = []\n",
    "        for letter in letters:\n",
    "            # \" A\", \" B\" ë“± ì‹œë„\n",
    "            tid = processor.tokenizer.encode(f\" {letter}\", add_special_tokens=False)\n",
    "            if tid:\n",
    "                token_ids.append(tid[0])\n",
    "            else:\n",
    "                tid = processor.tokenizer.encode(letter, add_special_tokens=False)\n",
    "                token_ids.append(tid[0])\n",
    "        \n",
    "        probs = torch.softmax(first_scores, dim=-1)[token_ids].cpu().numpy()\n",
    "    else:\n",
    "        probs = np.array([0.25] * 4)  # fallback\n",
    "    \n",
    "    target_idx = options.index(target_action)\n",
    "    score = probs[target_idx] * 1000000\n",
    "    \n",
    "    chosen = letters[np.argmax(probs)]\n",
    "    print(f\"Target: '{target_action}' | Score: {score:.4f} | Model chose: {chosen} ({options[letters.index(chosen)]})\")\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "41f29e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A) sleep\n",
      "B) jump\n",
      "C) carry\n",
      "D) drink\n",
      "Model raw output: 'D'\n",
      "Target: 'sleep' | Score: 0.0319 | Model chose: D (drink)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float32(0.031892024)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score_forced_choice(image_path, \"sleep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b788c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A) hold\n",
      "B) jump\n",
      "C) carry\n",
      "D) drink\n",
      "Model raw output: 'A'\n",
      "Target: 'hold' | Score: 2.4557 | Model chose: D (drink)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float32(2.4557068)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score_forced_choice(image_path, \"hold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8537386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image.org Forced-Choice Score for 'drink': 0.0000 (chosen: B)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float32(3.281454e-06)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score_forced_choice(image_path, \"drink\",distractors = [\"eat\", \"carry\", \"look at\", \"touch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "efd2b7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image.org Forced-Choice Score for 'eat': 0.0000 (chosen: C)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float32(1.1586e-07)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score_forced_choice(image_path, \"eat\",distractors = [\"drink\", \"carry\", \"look at\", \"touch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f9228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image.org Forced-Choice Score for 'drink': 0.0000 (chosen: D)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float32(1.8503738e-06)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score_forced_choice(image_path, \"drink\",distractors = [\"jump\", \"carry\", \"look at\", \"touch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c3c3fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_action_score(image_path, action_text):\n",
    "        \"\"\"\n",
    "        ìœ ì‚¬ë„(Cosine Similarity) ëŒ€ì‹ , \n",
    "        ì´ë¯¸ì§€ê°€ ì£¼ì–´ì¡Œì„ ë•Œ í•´ë‹¹ í…ìŠ¤íŠ¸(action)ê°€ ìƒì„±ë  'í™•ë¥ (Score)'ì„ êµ¬í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. í”„ë¡¬í”„íŠ¸ êµ¬ì„±: \"ì´ë¯¸ì§€ë¥¼ ë³´ê³  í–‰ë™ì„ ë§ì¶°ë¼\"ëŠ” ì‹ì˜ ìœ ë„\n",
    "        # ì˜ˆ: \"Describe the action in this image.\" -> ê¸°ëŒ€ ë‹µë³€: \"jump\"\n",
    "        prompt_text = \"Describe the action shown in the image in one word or phrase.\"\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image_path},\n",
    "                    {\"type\": \"text\", \"text\": prompt_text}\n",
    "                ]\n",
    "            },\n",
    "            # ì—¬ê¸°ì„œ ëª¨ë¸ì´ ìƒì„±í•´ì•¼ í•  ì •ë‹µ(Label)ì„ ë¯¸ë¦¬ ë„£ì–´ì¤ë‹ˆë‹¤.\n",
    "            {\"role\": \"assistant\", \"content\": action_text} \n",
    "        ]\n",
    "\n",
    "        # 2. ì…ë ¥ ì²˜ë¦¬\n",
    "        text = processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        \n",
    "        # process_vision_info ë“± ì „ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        # 3. Loss ê³„ì‚° (ì •ë‹µì„ ì•Œê³  ìˆì„ ë•Œì˜ ì†ì‹¤ê°’)\n",
    "        # labelsë¥¼ ì…ë ¥ê³¼ ë˜‘ê°™ì´ ì£¼ë©´, ëª¨ë¸ì´ input_idsë¥¼ ë³´ê³  ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë§ì¶”ëŠ” Lossë¥¼ ê³„ì‚°í•¨\n",
    "        # ë‹¨, ìš°ë¦¬ëŠ” \"ì§ˆë¬¸\" ë¶€ë¶„ì˜ LossëŠ” ë¬´ì‹œí•˜ê³  \"ë‹µë³€(action_text)\" ë¶€ë¶„ì˜ Lossë§Œ ë³´ê³  ì‹¶ìŒ.\n",
    "        # í•˜ì§€ë§Œ ê°„ë‹¨í•˜ê²Œ ì „ì²´ Lossë¥¼ ë´ë„, ë‹µë³€ì´ ì§§ë‹¤ë©´(action_text) ì¶©ë¶„íˆ ìœ ì˜ë¯¸í•¨.\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            \n",
    "            # LossëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •ë‹µ). \n",
    "            # ì´ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ì²˜ëŸ¼ ì“°ê¸° ìœ„í•´ ë³€í™˜: Score = exp(-loss)\n",
    "            # lossê°€ ë‚®ì„ìˆ˜ë¡ scoreëŠ” 1ì— ê°€ê¹Œì›Œì§\n",
    "            loss = outputs.loss\n",
    "            score = torch.exp(-loss).item()\n",
    "\n",
    "        score *= 1000000\n",
    "        # with torch.no_grad():\n",
    "        #     output = model.generate(\n",
    "        #         **inputs, \n",
    "        #         max_new_tokens=1024, \n",
    "        #         do_sample=False,\n",
    "        #         temperature=0.0 \n",
    "        #     )\n",
    "        # # 4) ê²°ê³¼ ë””ì½”ë”©\n",
    "        # # ê²°ê³¼ ë””ì½”ë”©\n",
    "        # generated_ids_trimmed = [\n",
    "        #     out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, output)\n",
    "        # ]\n",
    "        # print(f\"generated_ids_trimmed : {generated_ids_trimmed}\")\n",
    "        # response = processor.batch_decode(\n",
    "        #     generated_ids_trimmed, \n",
    "        #     skip_special_tokens=True, \n",
    "        #     clean_up_tokenization_spaces=False\n",
    "        # )[0]\n",
    "        # print(\"response : \",response)\n",
    "\n",
    "        print(f\"ğŸ“Š Generative Score ('{action_text}' vs Image): {score:.6f}\")\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b2d0cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/bongo/porter_notebook/research/WSAG-PLSP/AGD20K/Seen/trainset/exocentric/drink_with/wine_glass/drink_with_wine glass_001335.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7c9f49b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Generative Score ('hold' vs Image): 0.017770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.017770027227470564"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score(image_path, \"hold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9fb78dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Generative Score ('brush' vs Image): 0.017732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.017732174839579784"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score(image_path, \"brush\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "485db14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Generative Score ('cut' vs Image): 0.017624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01762383838865844"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score(image_path, \"cut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "345fde7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Generative Score ('drink' vs Image): 0.018078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.018077669139415775"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score(image_path, \"drink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6c457d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Generative Score ('run' vs Image): 0.017604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01760361278968503"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_score(image_path, \"run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cbae22d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Cosine Similarity ('sleep' vs Image): 0.820312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8203125"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_similarity(image_path, \"sleep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a2ffb198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Cosine Similarity ('cut' vs Image): 0.894531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.89453125"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_similarity(image_path, \"cut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a838820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Cosine Similarity ('brush' vs Image): 0.882812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8828125"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_similarity(image_path, \"brush\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5f666abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Cosine Similarity ('jump' vs Image): 0.867188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8671875"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_action_similarity(image_path, \"jump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f21e99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Cosine Similarity ('jump' vs Image): 0.546875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Qwen2.5-VL ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì™€ í–‰ë™ í…ìŠ¤íŠ¸(action) ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# 1. ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ (Image Context)\n",
    "# ==========================================\n",
    "# Qwen2.5-VLì€ process_vision_infoë¥¼ í†µí•´ ë™ì  í•´ìƒë„ ì²˜ë¦¬ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.\n",
    "image_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            # ì´ë¯¸ì§€ íŠ¹ì§•ì„ í™œì„±í™”í•˜ê¸° ìœ„í•´ ì¼ë°˜ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ê°™ì´ ë„£ìŠµë‹ˆë‹¤.\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"} \n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# A. í…ìŠ¤íŠ¸ í…œí”Œë¦¿ ì ìš©\n",
    "text_for_img = processor.apply_chat_template(\n",
    "    image_messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# B. Vision ì •ë³´ ì²˜ë¦¬ (ì´ê²Œ í•µì‹¬ì…ë‹ˆë‹¤: ì´ë¯¸ì§€ í† í° ê³µê°„ í™•ë³´)\n",
    "image_inputs, video_inputs = process_vision_info(image_messages)\n",
    "\n",
    "# C. Processor í†µê³¼\n",
    "inputs_img = processor(\n",
    "    text=[text_for_img],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs_img = inputs_img.to(device)\n",
    "\n",
    "# D. ëª¨ë¸ Forward (Generateê°€ ì•„ë‹˜)\n",
    "with torch.no_grad():\n",
    "    # output_hidden_states=Trueë¡œ ì„¤ì •í•˜ì—¬ ë²¡í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    outputs_img = model(**inputs_img, output_hidden_states=True)\n",
    "    # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ Hidden State (Batch, Seq, Dim) -> í‰ê·  í’€ë§ -> (Batch, Dim)\n",
    "    # ì´ë¯¸ì§€ ì •ë³´ê°€ í¬í•¨ëœ ë¬¸ë§¥ ì „ì²´ì˜ í‰ê·  ë²¡í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    img_embedding = outputs_img.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ (Text Context)\n",
    "# ==========================================\n",
    "# ë¹„êµí•  í–‰ë™(action) í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ì…ë ¥ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "text_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": action_text}]\n",
    "    }\n",
    "]\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë„ ë™ì¼í•œ í…œí”Œë¦¿ êµ¬ì¡°ë¥¼ ê±°ì³ì•¼ ì„ë² ë”© ê³µê°„ì´ ì •ë ¬ë©ë‹ˆë‹¤.\n",
    "text_for_txt = processor.apply_chat_template(\n",
    "    text_messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs_txt = processor(\n",
    "    text=[text_for_txt],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_txt = model(**inputs_txt, output_hidden_states=True)\n",
    "    txt_embedding = outputs_txt.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. ìœ ì‚¬ë„ ê³„ì‚° ë° ê²°ê³¼ ì¶œë ¥\n",
    "# ==========================================\n",
    "# ì°¨ì› í™•ì¸ (ë””ë²„ê¹…ìš©)\n",
    "# print(f\"âœ… Text Shape: {txt_embedding.shape}, Image Shape: {img_embedding.shape}\")\n",
    "\n",
    "similarity = F.cosine_similarity(txt_embedding, img_embedding)\n",
    "score = similarity.item()\n",
    "\n",
    "print(f\"ğŸ“Š Cosine Similarity ('{action_text}' vs Image): {score:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecdbb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs_img, \n",
    "        max_new_tokens=1024, \n",
    "        do_sample=False,\n",
    "        temperature=0.0 \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa383852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,  ..., 151644,  77091,    198]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_img.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6039e5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,  ...,    990,     13, 151645]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "601a0109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLCausalLMOutputWithPast(loss=None, logits=tensor([[[11.0625, 17.7500, 17.5000,  ...,  8.5625,  8.5625,  8.5625],\n",
       "         [10.3125, 15.1875,  9.5000,  ...,  6.8750,  6.8750,  6.8750],\n",
       "         [15.3125, 18.2500, 20.6250,  ...,  6.7812,  6.7812,  6.7812],\n",
       "         ...,\n",
       "         [ 9.3750,  9.9375,  2.8906,  ...,  8.6250,  8.6250,  8.6250],\n",
       "         [15.2500, 14.1250, 12.8750,  ...,  6.4688,  6.4688,  6.4688],\n",
       "         [17.0000, 17.8750, 16.5000,  ...,  7.3125,  7.3125,  7.3125]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), past_key_values=<transformers.cache_utils.DynamicCache object at 0x7fc441529850>, hidden_states=(tensor([[[-0.0232,  0.0008, -0.0154,  ...,  0.0149,  0.0042,  0.0001],\n",
       "         [-0.0308,  0.0371,  0.0289,  ..., -0.0188,  0.0061,  0.0212],\n",
       "         [ 0.0330, -0.0216, -0.0021,  ..., -0.0209, -0.0210, -0.0383],\n",
       "         ...,\n",
       "         [-0.0232,  0.0008, -0.0154,  ...,  0.0149,  0.0042,  0.0001],\n",
       "         [-0.0107, -0.0155, -0.0082,  ..., -0.0176,  0.0081,  0.0287],\n",
       "         [ 0.0330, -0.0216, -0.0021,  ..., -0.0209, -0.0210, -0.0383]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.1426,  0.2930,  0.1592,  ..., -0.8164,  0.1230,  0.0615],\n",
       "         [-0.2598,  0.5156,  0.6211,  ...,  0.2256,  0.0840, -0.2070],\n",
       "         [-0.3320,  0.0107,  0.0791,  ...,  0.0630,  0.2988, -0.2949],\n",
       "         ...,\n",
       "         [ 0.3359, -0.1235,  0.4785,  ..., -0.6992, -0.3691,  0.5117],\n",
       "         [ 0.4180,  0.1504, -0.2715,  ..., -0.2969, -0.4453,  0.6328],\n",
       "         [-0.0317, -0.1953,  0.0884,  ...,  0.1201,  0.1602, -0.2070]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.6758,  0.1875,  0.0527,  ..., -1.0156, -0.6406,  1.3125],\n",
       "         [-0.4512,  0.5469,  0.5859,  ...,  0.5078,  0.0586,  0.1533],\n",
       "         [-0.4238,  0.1855, -0.2178,  ...,  0.3379,  0.4238,  0.0957],\n",
       "         ...,\n",
       "         [ 0.1006,  0.2832,  0.4902,  ..., -0.5664, -0.0625,  0.9922],\n",
       "         [ 0.0864,  0.0747, -0.1289,  ..., -0.2539, -0.2695,  0.6562],\n",
       "         [-0.1768, -0.1553, -0.0104,  ...,  0.0352,  0.3320, -0.1826]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 1.9219,  3.1719, -3.6250,  ..., -1.1250, -0.5078,  2.7500],\n",
       "         [-0.7109,  0.4531,  0.6133,  ...,  0.7852,  0.2266,  0.2207],\n",
       "         [-0.3809,  0.4590, -0.0596,  ...,  0.5039,  0.2871, -0.0723],\n",
       "         ...,\n",
       "         [-0.2246,  0.1836,  0.7266,  ..., -0.2080, -0.0459,  0.7695],\n",
       "         [-0.1348,  0.0552, -0.0996,  ..., -0.0410, -0.2891,  0.5898],\n",
       "         [-0.1162, -0.0454,  0.1133,  ...,  0.0187,  0.5234, -0.2520]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 1.6094,  2.5938, -2.4375,  ..., -0.6719,  1.5312,  1.2969],\n",
       "         [-0.6680,  0.5742,  0.8203,  ...,  0.7734,  0.2520,  0.0596],\n",
       "         [-0.0312,  0.5391, -0.2578,  ...,  0.3262,  0.1484, -0.2031],\n",
       "         ...,\n",
       "         [-0.3945,  0.2109,  0.6875,  ..., -0.0586, -0.3672,  0.3535],\n",
       "         [ 0.0977,  0.0376, -0.0864,  ..., -0.0195, -0.2363,  0.4961],\n",
       "         [ 0.1113,  0.0078,  0.1113,  ...,  0.1260,  0.6250,  0.2812]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.9727,  1.9531, -1.6719,  ..., -1.1562,  1.4531,  0.5625],\n",
       "         [-0.7891,  0.0039,  0.8867,  ...,  0.7539, -0.0332,  0.0112],\n",
       "         [-0.0918,  0.4961, -0.3711,  ...,  0.2324,  0.2480, -0.2539],\n",
       "         ...,\n",
       "         [-0.5039,  0.2285,  0.4023,  ..., -0.3516, -0.5000,  0.1826],\n",
       "         [ 0.1035, -0.1738, -0.3828,  ..., -0.1006,  0.1670,  0.1768],\n",
       "         [-0.0615,  0.0791,  0.0376,  ...,  0.1348,  0.6797,  0.3223]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.8164,  1.8672, -1.3594,  ..., -1.0469,  1.4062,  0.4141],\n",
       "         [-0.9102, -0.0273,  0.5625,  ...,  0.5156, -0.2734,  0.3926],\n",
       "         [-0.2617,  0.5820, -0.0693,  ...,  0.2695,  0.0381, -0.2412],\n",
       "         ...,\n",
       "         [-0.3125,  0.1582,  0.1562,  ...,  0.0566, -0.9453, -0.0820],\n",
       "         [ 0.7578, -1.0078, -0.1074,  ..., -0.1963,  0.1172,  0.0234],\n",
       "         [-0.2285,  0.1123,  0.1631,  ...,  0.2021,  0.6797,  0.2910]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 1.0859,  2.0938, -1.3906,  ..., -0.7891,  1.2500,  0.2461],\n",
       "         [-1.1406,  0.3516,  0.2344,  ...,  1.0156, -0.2578,  1.1875],\n",
       "         [-0.2910,  0.6484,  0.4141,  ...,  0.5469, -0.0127, -0.0254],\n",
       "         ...,\n",
       "         [-0.7266,  0.1416,  0.1641,  ...,  0.2227, -1.2656,  0.0811],\n",
       "         [ 0.1807, -1.0469, -0.0264,  ...,  0.1235, -0.1279, -0.4453],\n",
       "         [-0.1895,  0.2129,  0.3730,  ...,  0.2500,  0.3594,  0.2129]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 1.3516,  2.6250, -1.3359,  ..., -0.4473,  1.2422, -0.1738],\n",
       "         [-0.4766,  0.2041,  0.3633,  ...,  0.7734, -0.4609,  0.9141],\n",
       "         [-0.0059,  0.1699,  0.2031,  ...,  0.7031, -0.2812,  0.0781],\n",
       "         ...,\n",
       "         [ 0.0830,  0.0410, -0.0332,  ...,  0.1406, -1.6406,  0.2578],\n",
       "         [ 0.4961, -2.1250, -1.0312,  ...,  1.0938, -0.1953, -1.0078],\n",
       "         [-0.0352, -0.2051,  0.0312,  ...,  0.0391,  0.6133,  0.2930]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 1.3125,  3.0625, -1.3359,  ..., -0.2363,  1.1406, -0.5469],\n",
       "         [-0.6641,  0.1855,  0.0693,  ...,  0.7109, -0.3340,  0.9883],\n",
       "         [ 0.4219, -0.2129,  0.1973,  ...,  0.6133, -0.1318, -0.4297],\n",
       "         ...,\n",
       "         [ 0.0625, -0.1904, -0.3477,  ...,  0.4570, -1.6094, -0.1143],\n",
       "         [-0.1016, -2.1250, -2.1562,  ...,  1.3281, -0.3398, -1.4062],\n",
       "         [-0.0303, -0.4355, -0.2031,  ...,  0.1953,  0.4023,  0.1201]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 2.0625,  3.5000, -1.3047,  ...,  0.0957,  0.9961, -0.6523],\n",
       "         [-0.5430,  0.1875,  0.3242,  ...,  1.3125, -0.6562,  0.9219],\n",
       "         [ 0.7031, -0.4238,  0.2715,  ...,  0.8789,  0.0967, -0.5938],\n",
       "         ...,\n",
       "         [-0.1152, -0.4746, -0.3633,  ...,  0.6680, -1.6562, -0.1797],\n",
       "         [ 0.3574, -2.0625, -1.7266,  ...,  1.6719, -0.6797, -0.9453],\n",
       "         [ 0.7500, -0.7734, -0.7188,  ...,  0.5352,  1.0312,  0.4922]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 1.8203e+00,  3.7656e+00, -1.3203e+00,  ..., -6.6406e-02,\n",
       "           1.2656e+00, -4.7656e-01],\n",
       "         [-5.3906e-01, -2.0801e-01,  2.5195e-01,  ...,  7.1875e-01,\n",
       "          -5.1562e-01,  9.8438e-01],\n",
       "         [ 4.5508e-01, -5.6641e-02, -4.7852e-02,  ...,  8.0859e-01,\n",
       "           4.6875e-01,  3.9062e-03],\n",
       "         ...,\n",
       "         [-1.0781e+00, -4.4336e-01, -6.2500e-01,  ...,  5.5078e-01,\n",
       "          -2.9297e-01,  2.5781e-01],\n",
       "         [ 1.4160e-01, -2.1719e+00, -2.5000e+00,  ...,  1.4375e+00,\n",
       "           1.3281e-01, -1.8750e+00],\n",
       "         [ 1.2734e+00, -1.1523e-01, -3.5156e-02,  ...,  1.5781e+00,\n",
       "           1.4297e+00,  1.9531e-03]]], device='cuda:0', dtype=torch.bfloat16), tensor([[[ 2.1719e+00,  3.7031e+00, -1.4922e+00,  ...,  2.9297e-03,\n",
       "           1.4219e+00, -5.9766e-01],\n",
       "         [-7.6953e-01, -6.8750e-01,  3.8672e-01,  ...,  2.3047e-01,\n",
       "          -1.0312e+00,  2.2656e-01],\n",
       "         [-3.6523e-01,  1.8652e-01,  3.1641e-01,  ...,  3.0664e-01,\n",
       "           9.4531e-01, -2.1875e-01],\n",
       "         ...,\n",
       "         [-7.2656e-01, -7.6953e-01, -4.1797e-01,  ..., -4.0039e-02,\n",
       "           4.8047e-01, -5.7812e-01],\n",
       "         [-6.0059e-02, -2.1250e+00, -1.2266e+00,  ...,  2.3438e-02,\n",
       "          -1.7188e-01, -1.3594e+00],\n",
       "         [ 7.5781e-01, -4.5508e-01, -3.6328e-01,  ...,  1.2500e+00,\n",
       "           9.3750e-01, -5.7812e-01]]], device='cuda:0', dtype=torch.bfloat16), tensor([[[ 2.4375,  3.4844, -1.3125,  ..., -0.0483,  0.9766, -0.6484],\n",
       "         [-0.9648, -0.3047, -0.3730,  ...,  0.6172, -0.7656,  0.3223],\n",
       "         [-0.2695,  0.2256,  0.6641,  ...,  0.1660,  0.7031, -0.0781],\n",
       "         ...,\n",
       "         [-1.6250, -1.1719, -0.7969,  ..., -0.0144, -0.9219, -0.7461],\n",
       "         [ 0.5078, -1.4844, -1.5000,  ...,  0.4922, -0.2891, -1.4062],\n",
       "         [ 0.2383, -0.1465,  0.0918,  ...,  1.0000,  1.3906, -0.1484]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 2.6094,  3.6719, -1.2188,  ..., -0.3750,  1.3828, -0.7656],\n",
       "         [-1.0312, -0.1982, -1.0391,  ...,  0.7539,  0.0000,  0.5391],\n",
       "         [-0.7500,  0.5000, -0.1797,  ...,  0.3906,  0.8984, -0.0742],\n",
       "         ...,\n",
       "         [-1.4688, -1.4062, -0.8398,  ..., -0.4082, -0.6836,  0.1602],\n",
       "         [ 0.8867, -2.6719, -1.0938,  ...,  0.4453, -0.9219, -2.2812],\n",
       "         [-0.1377, -0.2188,  0.8438,  ...,  0.7930,  1.9453, -1.3438]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 2.5000,  3.6875, -0.8672,  ..., -0.2871,  1.8203, -1.1172],\n",
       "         [-0.6680, -0.5430, -1.3516,  ...,  1.0469, -0.1602,  0.3242],\n",
       "         [-0.3789,  0.4141, -0.4414,  ...,  0.2070,  0.7656, -0.1777],\n",
       "         ...,\n",
       "         [-1.3359, -0.6602, -0.5508,  ..., -0.4785, -0.3555,  0.5703],\n",
       "         [ 2.7188, -2.1250, -1.6875,  ...,  0.6172, -1.0391, -2.1250],\n",
       "         [ 0.6211, -0.0410,  0.3926,  ...,  0.4219,  1.5156, -0.5273]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 2.5156,  3.6094, -0.6875,  ..., -0.7031,  1.7812, -1.4844],\n",
       "         [-0.5938, -0.2930, -1.9375,  ...,  0.6094, -0.4785, -0.0537],\n",
       "         [-1.2188,  0.0820, -0.7344,  ..., -0.3809,  0.8750, -0.0605],\n",
       "         ...,\n",
       "         [-0.9922, -0.1992, -0.4258,  ..., -0.0137,  0.6836,  0.3066],\n",
       "         [ 1.5000, -1.7969, -0.9766,  ...,  0.5703, -0.0391, -1.8281],\n",
       "         [ 0.0078,  0.2441,  0.2002,  ...,  0.1523,  2.1250, -0.2578]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 2.6719,  3.2812, -0.4668,  ..., -0.9492,  1.4844, -1.4766],\n",
       "         [-0.4102, -0.0693, -1.8125,  ...,  0.1953, -0.4648,  0.0479],\n",
       "         [-1.2500,  0.4766, -0.4199,  ..., -0.8828,  0.3516,  0.0132],\n",
       "         ...,\n",
       "         [-1.6719,  0.3867, -0.3809,  ..., -0.1670,  1.2422, -0.0752],\n",
       "         [ 0.7383, -2.3594,  0.1094,  ...,  0.2305, -0.0781, -1.6641],\n",
       "         [-0.1240,  0.3086,  0.1104,  ...,  0.3301,  1.6406, -0.4219]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 2.8906,  3.5781,  0.2930,  ..., -1.1953,  1.6094, -1.5547],\n",
       "         [-0.5664, -0.2275, -1.3984,  ...,  0.2930, -0.5156, -0.2930],\n",
       "         [-1.2188,  0.1914,  0.1055,  ..., -0.8125,  0.6211,  0.2891],\n",
       "         ...,\n",
       "         [-1.6328,  1.0078, -0.2695,  ...,  0.1250,  1.2656,  0.1328],\n",
       "         [ 1.1641, -2.4688,  0.3066,  ...,  0.0278,  0.9180, -1.8672],\n",
       "         [ 0.3672, -0.0410, -0.0574,  ...,  0.0352,  2.0312, -0.4434]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 2.9688,  3.5781,  0.7656,  ..., -1.2578,  1.6250, -1.7344],\n",
       "         [ 0.0273,  0.2773, -1.7812,  ...,  0.6914, -0.4082, -0.6914],\n",
       "         [-1.2266, -0.1250, -0.1387,  ..., -0.5938,  0.7109,  0.3398],\n",
       "         ...,\n",
       "         [-1.5312,  0.9609, -0.0391,  ...,  0.0186,  1.7969,  0.1699],\n",
       "         [ 0.4336, -1.7656, -0.1475,  ...,  0.1680,  0.6719, -0.7422],\n",
       "         [ 0.0840,  0.0303, -0.0527,  ..., -0.4297,  2.0312, -0.5625]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 3.1094,  3.7812,  0.8984,  ..., -1.4531,  1.6250, -1.9297],\n",
       "         [ 0.1953,  0.1523, -2.4062,  ...,  1.3203, -0.6094,  0.0312],\n",
       "         [-0.9375, -0.8281, -0.2031,  ..., -0.3203,  1.6094,  0.0977],\n",
       "         ...,\n",
       "         [-1.7266,  0.7188,  0.5000,  ..., -0.5039,  2.0469, -0.1094],\n",
       "         [ 0.5703, -1.3516,  0.1133,  ...,  0.5898,  0.2451, -1.2578],\n",
       "         [ 0.4727, -0.2354, -0.3945,  ..., -0.1074,  1.7344, -0.7344]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 3.2500,  3.7812,  0.9766,  ..., -1.6406,  1.5781, -2.0312],\n",
       "         [ 0.0190,  0.4062, -2.4375,  ...,  0.9609, -0.7695,  0.7852],\n",
       "         [-0.1953, -1.1172, -0.7227,  ..., -0.6094,  1.4375,  0.8281],\n",
       "         ...,\n",
       "         [-1.2656,  0.8398,  0.3633,  ..., -0.0703,  2.4219, -0.5000],\n",
       "         [ 0.5430, -1.4453, -0.0527,  ...,  1.2500,  0.1318, -1.1641],\n",
       "         [ 0.8828,  0.2676, -0.7930,  ..., -0.4180,  1.6250, -0.5820]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 3.2031,  4.1562,  0.9180,  ..., -1.7500,  1.7188, -2.1875],\n",
       "         [-0.5078,  0.2109, -2.8750,  ...,  0.8047, -0.1914,  0.1133],\n",
       "         [-0.1113, -0.9375, -0.9297,  ...,  0.0059,  1.6797,  0.5078],\n",
       "         ...,\n",
       "         [-1.1875,  0.2500,  0.2246,  ..., -0.5078,  2.5312,  0.1172],\n",
       "         [ 1.3672, -2.0469,  0.9844,  ...,  0.5000,  0.1982, -1.4219],\n",
       "         [ 0.9688,  0.1348,  0.4375,  ..., -0.7969,  1.2031, -1.1016]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 3.1406,  4.3750,  0.9883,  ..., -1.6953,  1.7969, -2.3125],\n",
       "         [-0.5352,  0.6641, -3.5156,  ...,  0.8594, -0.1797,  0.0090],\n",
       "         [-0.0557, -0.6328, -0.6328,  ..., -0.3770,  2.0469,  0.2773],\n",
       "         ...,\n",
       "         [-1.5859,  0.6055,  0.2949,  ..., -0.7891,  2.2969,  0.5000],\n",
       "         [ 1.1328, -1.5781,  0.6289,  ...,  0.7305,  0.6289, -1.7500],\n",
       "         [ 0.7930, -0.6289,  0.4961,  ..., -0.8398,  1.9531, -0.7188]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 3.2344,  4.1875,  1.0938,  ..., -1.3984,  1.8125, -2.3594],\n",
       "         [-0.8359, -0.6289, -3.6406,  ...,  1.5312, -0.1699,  0.0244],\n",
       "         [-0.4160, -2.0469, -0.3438,  ..., -0.1201,  1.8203, -0.3945],\n",
       "         ...,\n",
       "         [-1.3906,  0.8672,  0.2891,  ..., -0.1836,  1.6953,  0.0957],\n",
       "         [-0.1719, -1.1406,  1.2344,  ...,  0.6406,  1.3438, -1.4453],\n",
       "         [ 0.9805,  0.2422,  0.8047,  ...,  0.3320,  2.2344, -1.1562]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 3.3281,  4.0000,  1.2344,  ..., -1.2578,  1.8281, -2.3594],\n",
       "         [-0.3711, -0.8438, -3.6875,  ...,  1.8281, -1.0078, -0.3848],\n",
       "         [-0.0137, -2.2500, -0.6016,  ...,  0.1533,  2.5781, -0.9375],\n",
       "         ...,\n",
       "         [-0.9297,  0.4531, -0.2930,  ..., -0.7031,  1.8281,  0.5352],\n",
       "         [ 0.7148, -1.8594,  1.1328,  ...,  0.2041,  1.7266, -1.2188],\n",
       "         [ 0.7969, -0.1406,  0.0234,  ..., -0.8789,  1.7812, -0.2930]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 3.1562,  3.6562,  2.0938,  ..., -1.1562,  1.3359, -2.0625],\n",
       "         [-1.1250, -0.4062, -4.2812,  ...,  0.7852, -0.3945, -0.3594],\n",
       "         [-0.3477, -3.0625, -1.0781,  ..., -0.0146,  2.2500, -0.8594],\n",
       "         ...,\n",
       "         [-0.7969,  0.5352, -0.8672,  ..., -1.2656,  1.4766,  0.8867],\n",
       "         [ 1.3125, -1.8594,  0.3164,  ..., -0.7227,  2.4688, -2.3906],\n",
       "         [-0.2148, -0.0957, -0.7461,  ..., -1.2656,  0.9336, -0.7617]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 3.3750,  3.5781,  2.2031,  ..., -1.7031,  1.5078, -2.0625],\n",
       "         [-2.0469, -0.5547, -3.4531,  ...,  0.0781, -0.7500, -0.6758],\n",
       "         [-0.5117, -2.6562, -0.2656,  ..., -0.5547,  2.9375, -0.5664],\n",
       "         ...,\n",
       "         [-1.6875, -0.5938, -0.4062,  ..., -1.8438,  1.8203,  0.8359],\n",
       "         [ 1.0781, -1.3203, -0.0879,  ..., -1.1094,  2.3438, -2.6250],\n",
       "         [ 0.0420,  0.3320, -0.4434,  ..., -1.0547,  0.0586, -2.1250]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 3.8906,  2.6719,  2.6250,  ..., -1.3438,  2.3125, -1.3750],\n",
       "         [-1.9453, -0.7500, -4.0625,  ..., -0.4648,  0.7852, -1.0391],\n",
       "         [-0.2617, -2.4531, -0.1592,  ..., -0.3906,  4.0000, -1.1016],\n",
       "         ...,\n",
       "         [-3.1250, -2.0625, -1.5938,  ..., -1.9531,  1.9375,  0.0508],\n",
       "         [ 1.5156, -0.7344,  0.1089,  ..., -0.9727,  3.5625, -2.3438],\n",
       "         [ 1.5469, -0.7734, -0.4531,  ..., -0.9219,  0.8398, -0.0859]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 4.5000,  2.5469,  3.1719,  ..., -0.8047,  2.8438, -2.1250],\n",
       "         [-4.0312, -0.9375, -3.9375,  ...,  1.2344,  1.2266,  0.5078],\n",
       "         [-0.1040, -1.5469,  0.8945,  ...,  0.8828,  3.6719,  0.1719],\n",
       "         ...,\n",
       "         [-2.2500, -2.3594, -2.2500,  ..., -2.7812,  2.9844, -1.3047],\n",
       "         [ 1.6406, -2.4375,  0.1377,  ..., -1.3438,  2.9844, -2.4844],\n",
       "         [-0.2969, -1.2656,  0.2227,  ..., -1.1719, -0.0312,  0.2422]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 6.0000,  1.6875,  4.5312,  ..., -0.6055,  1.8281, -2.5781],\n",
       "         [-3.0000, -1.2891, -4.7500,  ...,  1.5703, -0.1406, -0.4805],\n",
       "         [ 0.9922, -0.9062,  0.7461,  ...,  0.5469,  2.3906,  1.2266],\n",
       "         ...,\n",
       "         [-1.5469, -2.8281, -2.7344,  ..., -1.3203,  2.8281, -0.9492],\n",
       "         [ 1.6875, -3.3594, -0.8047,  ..., -3.0312,  2.4844, -1.6406],\n",
       "         [ 0.0293, -5.1562, -0.0840,  ..., -1.4375,  0.5742,  2.2969]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 4.0312, -0.9062,  0.5469,  ...,  3.0625,  1.2969, -0.2500],\n",
       "         [-4.9375, -1.3047, -2.3594,  ..., -0.0234, -1.8828, -1.3438],\n",
       "         [ 1.8438, -2.4375, -1.3438,  ..., -1.1719,  1.9766,  2.0625],\n",
       "         ...,\n",
       "         [ 0.6016, -2.6875, -2.9688,  ..., -1.8672,  3.2188, -1.6719],\n",
       "         [ 1.8047, -4.4062, -0.7812,  ..., -4.7500,  4.3125, -1.1250],\n",
       "         [ 2.0312, -5.4688, -0.8047,  ..., -1.7188,  0.2422,  3.9375]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 3.0469, -0.8984,  0.6641,  ...,  9.1250,  3.2188, -2.5625],\n",
       "         [-7.6250, -0.2871, -2.8125,  ...,  0.7500, -8.1875, -5.4688],\n",
       "         [ 2.6406, -2.7969, -1.6875,  ...,  0.0981,  2.0312,  0.7578],\n",
       "         ...,\n",
       "         [ 1.2969, -2.3438, -1.8438,  ..., -1.9531,  1.6875, -1.3906],\n",
       "         [ 1.3828, -3.9844, -0.5742,  ..., -4.8750,  4.9062, -2.0625],\n",
       "         [ 2.5312, -4.2500, -0.8398,  ..., -1.0312,  0.5938,  1.6875]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ -0.0117,  -1.7500,  -0.3555,  ...,   8.9375,  -0.5234,  -4.3125],\n",
       "         [ -9.5000,  -3.5625,  -4.3438,  ...,  -0.1992, -10.5625,  -6.5625],\n",
       "         [  3.6562,  -4.2812,  -1.5469,  ...,   0.2061,   2.2500,  -0.1484],\n",
       "         ...,\n",
       "         [  0.3125,  -2.8594,  -1.7500,  ...,  -3.9062,   0.1875,  -1.3516],\n",
       "         [ -0.7188,  -3.5625,  -0.6992,  ...,  -5.5625,   5.3438,  -1.3203],\n",
       "         [  2.7969,  -3.4688,  -1.5781,  ...,  -0.9961,   0.9375,   3.3750]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 3.7500e+00, -1.9688e+00, -2.3906e+00,  ...,  9.5000e+00,\n",
       "           3.0000e+00, -7.8750e+00],\n",
       "         [-4.2500e+00, -3.1562e+00, -7.8750e+00,  ..., -4.1250e+00,\n",
       "          -1.9688e+00, -1.1875e+01],\n",
       "         [ 1.6406e+00, -3.7812e+00, -8.0078e-01,  ...,  7.8125e-03,\n",
       "           2.4062e+00, -1.3379e-01],\n",
       "         ...,\n",
       "         [-2.7344e-02, -2.5938e+00, -3.6250e+00,  ..., -5.2500e+00,\n",
       "           4.8438e-01, -2.6875e+00],\n",
       "         [ 8.7109e-01, -3.8594e+00, -2.8750e+00,  ..., -6.3125e+00,\n",
       "           3.8750e+00, -1.8438e+00],\n",
       "         [ 9.4531e-01, -2.0312e+00, -4.7812e+00,  ..., -2.5625e+00,\n",
       "           2.8594e+00,  1.6719e+00]]], device='cuda:0', dtype=torch.bfloat16), tensor([[[  3.7500,  -4.3125,  -3.2188,  ...,   9.2500,   4.6875,  -4.5000],\n",
       "         [ -1.4844,  -5.5000,  -7.4375,  ...,  -3.7031,  -2.8125, -13.2500],\n",
       "         [  3.2969,  -3.8281,   0.5781,  ...,  -0.2598,   2.3594,   0.6211],\n",
       "         ...,\n",
       "         [  1.5156,  -0.7500,  -0.8594,  ...,  -5.3438,  -1.0078,  -3.2812],\n",
       "         [  2.2812,  -5.4375,  -2.2188,  ...,  -7.2188,   2.9688,  -2.6875],\n",
       "         [  2.4219,  -2.3125,  -2.5938,  ...,  -3.9844,   1.4688,  -1.4062]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16), tensor([[[ 0.2080, -3.1875, -0.0537,  ...,  4.9062,  2.8125, -2.5625],\n",
       "         [-2.1875, -3.7656,  0.6562,  ..., -0.8125, -4.1250, -6.4375],\n",
       "         [ 1.0859, -3.5469,  0.6445,  ..., -1.8438,  4.8125,  0.2148],\n",
       "         ...,\n",
       "         [ 0.8906, -3.5469,  1.8047,  ..., -3.2656, -4.1875, -2.3438],\n",
       "         [ 1.1484, -5.2188,  3.4219,  ..., -2.6562, -0.4590, -1.9453],\n",
       "         [-0.0496, -3.6250,  1.6016,  ...,  0.7422,  2.1094, -2.5469]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)), attentions=None, rope_deltas=tensor([[-1026]], device='cuda:0'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e188b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The image shows two skis standing upright against a white background. The skis have a light blue color with a gradient effect, transitioning from a lighter shade at the top to a darker shade at the bottom. Both skis feature the word \"TRAVEL\" written in red uppercase letters on the top surface. Below the word \"TRAVEL,\" there is a small logo consisting of three triangles forming an equilateral triangle. The ski tips and tails also have the same gradient color scheme but with a slightly different shade. The overall design is sleek and modern, suggesting that these skis are likely intended for advanced or professional use.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê²°ê³¼ ë””ì½”ë”©\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs_img.input_ids, output)\n",
    "]\n",
    "\n",
    "result = processor.batch_decode(\n",
    "    generated_ids_trimmed, \n",
    "    skip_special_tokens=True, \n",
    "    clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "31ec8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ê¸°ì¡´ í´ë˜ìŠ¤ ë‚´ë¶€ì— ì´ ë©”ì„œë“œë¥¼ ì¶”ê°€í•˜ì„¸ìš”\n",
    "def calculate_action_similarity(image_path, action_text):\n",
    "    \"\"\"\n",
    "    Qwen2.5-VL ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì™€ í–‰ë™ í…ìŠ¤íŠ¸(action) ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ==========================================\n",
    "    # 1. ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ (Image Context)\n",
    "    # ==========================================\n",
    "    # Qwen2.5-VLì€ process_vision_infoë¥¼ í†µí•´ ë™ì  í•´ìƒë„ ì²˜ë¦¬ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.\n",
    "    image_messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path},\n",
    "                # ì´ë¯¸ì§€ íŠ¹ì§•ì„ í™œì„±í™”í•˜ê¸° ìœ„í•´ ì¼ë°˜ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ê°™ì´ ë„£ìŠµë‹ˆë‹¤.\n",
    "                {\"type\": \"text\", \"text\": \"Describe the actions in the image.\"} \n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # A. í…ìŠ¤íŠ¸ í…œí”Œë¦¿ ì ìš©\n",
    "    text_for_img = processor.apply_chat_template(\n",
    "        image_messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # B. Vision ì •ë³´ ì²˜ë¦¬ (ì´ê²Œ í•µì‹¬ì…ë‹ˆë‹¤: ì´ë¯¸ì§€ í† í° ê³µê°„ í™•ë³´)\n",
    "    image_inputs, video_inputs = process_vision_info(image_messages)\n",
    "    \n",
    "    # C. Processor í†µê³¼\n",
    "    inputs_img = processor(\n",
    "        text=[text_for_img],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs_img = inputs_img.to(device)\n",
    "    \n",
    "    # D. ëª¨ë¸ Forward (Generateê°€ ì•„ë‹˜)\n",
    "    with torch.no_grad():\n",
    "        # output_hidden_states=Trueë¡œ ì„¤ì •í•˜ì—¬ ë²¡í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "        outputs_img = model(**inputs_img, output_hidden_states=True)\n",
    "        # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ Hidden State (Batch, Seq, Dim) -> í‰ê·  í’€ë§ -> (Batch, Dim)\n",
    "        # ì´ë¯¸ì§€ ì •ë³´ê°€ í¬í•¨ëœ ë¬¸ë§¥ ì „ì²´ì˜ í‰ê·  ë²¡í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "        # img_embedding = outputs_img.hidden_states[-1].mean(dim=1)\n",
    "        img_embedding = outputs_img.hidden_states[-1][:, -1, :]\n",
    "\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ (Text Context)\n",
    "    # ==========================================\n",
    "    # ë¹„êµí•  í–‰ë™(action) í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ì…ë ¥ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "    text_messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": action_text}]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ë„ ë™ì¼í•œ í…œí”Œë¦¿ êµ¬ì¡°ë¥¼ ê±°ì³ì•¼ ì„ë² ë”© ê³µê°„ì´ ì •ë ¬ë©ë‹ˆë‹¤.\n",
    "    text_for_txt = processor.apply_chat_template(\n",
    "        text_messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs_txt = processor(\n",
    "        text=[text_for_txt],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs_txt = model(**inputs_txt, output_hidden_states=True)\n",
    "        # txt_embedding = outputs_txt.hidden_states[-1].mean(dim=1)\n",
    "        txt_embedding = outputs_txt.hidden_states[-1][:, -1, :]\n",
    "\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. ìœ ì‚¬ë„ ê³„ì‚° ë° ê²°ê³¼ ì¶œë ¥\n",
    "    # ==========================================\n",
    "    # ì°¨ì› í™•ì¸ (ë””ë²„ê¹…ìš©)\n",
    "    # print(f\"âœ… Text Shape: {txt_embedding.shape}, Image Shape: {img_embedding.shape}\")\n",
    "\n",
    "    similarity = F.cosine_similarity(txt_embedding, img_embedding)\n",
    "    score = similarity.item()\n",
    "    \n",
    "    print(f\"ğŸ“Š Cosine Similarity ('{action_text}' vs Image): {score:.6f}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "# --- ì‚¬ìš©ë²• ì˜ˆì‹œ ---\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfe7b70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/DATA/AGD20K/Seen/testset/egocentric/jump/skis/skis_002829.jpg'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Cosine Similarity ('jump' vs Image): 0.546875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.546875"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = calculate_action_similarity(image_path, \"jump\")\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "feb1a3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Cosine Similarity ('ski' vs Image): 0.558594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55859375"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = calculate_action_similarity(image_path, \"ski\")\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9dafef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Cosine Similarity ('eat' vs Image): 0.511719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.51171875"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = calculate_action_similarity(image_path, \"eat\")\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739b476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Cosine Similarity ('brush' vs Image): 0.531250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.53125"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = calculate_action_similarity(image_path, \"brush\")\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a92e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b34163b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Image features and image tokens do not match: tokens: 0, features 1080",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     text_embedding \u001b[38;5;241m=\u001b[39m text_outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# --- ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ ---\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# ì´ë¯¸ì§€ë¥¼ LLMì— í†µê³¼ì‹œì¼œ í…ìŠ¤íŠ¸ ê³µê°„ìœ¼ë¡œ íˆ¬ì˜ëœ(projected) ë²¡í„°ë¥¼ ì–»ìŠµë‹ˆë‹¤.\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     image_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimage_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     image_embedding \u001b[38;5;241m=\u001b[39m image_outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 5. ì°¨ì› í™•ì¸ ë° ìœ ì‚¬ë„ ê³„ì‚°\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen25/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen25/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen25/lib/python3.9/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen25/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1487\u001b[0m, in \u001b[0;36mQwen2_5_VLForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **kwargs)\u001b[0m\n\u001b[1;32m   1482\u001b[0m output_attentions \u001b[38;5;241m=\u001b[39m output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_attentions\n\u001b[1;32m   1483\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1484\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1485\u001b[0m )\n\u001b[0;32m-> 1487\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecond_per_grid_ts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1507\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen25/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen25/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/qwen25/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1233\u001b[0m, in \u001b[0;36mQwen2_5_VLModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m n_image_features \u001b[38;5;241m=\u001b[39m image_embeds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling() \u001b[38;5;129;01mand\u001b[39;00m n_image_tokens \u001b[38;5;241m!=\u001b[39m n_image_features:\n\u001b[0;32m-> 1233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage features and image tokens do not match: tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_image_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, features \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_image_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1235\u001b[0m     )\n\u001b[1;32m   1237\u001b[0m mask \u001b[38;5;241m=\u001b[39m input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_token_id\n\u001b[1;32m   1238\u001b[0m mask_unsqueezed \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Image features and image tokens do not match: tokens: 0, features 1080"
     ]
    }
   ],
   "source": [
    "# 3. ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# A. í…ìŠ¤íŠ¸ ì…ë ¥ ì²˜ë¦¬ (\"jump\")\n",
    "text_inputs = processor(text=action, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# B. ì´ë¯¸ì§€ ì…ë ¥ ì²˜ë¦¬\n",
    "# Qwen-VLì€ ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•´ íŠ¹ë³„í•œ í† í°ì´ë‚˜ í¬ë§·ì´ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜, \n",
    "# ìµœì‹  AutoProcessorëŠ” images ì¸ìë¥¼ í†µí•´ ì´ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "image_inputs = processor(text=\"<image>\", images=image, return_tensors=\"pt\").to(device)\n",
    "# (ì°¸ê³ : ëª¨ë¸ ë²„ì „ì— ë”°ë¼ text promptì— <image> í† í°ì„ ëª…ì‹œí•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤)\n",
    "\n",
    "# 4. ì„ë² ë”© ì¶”ì¶œ (Forward Pass)\n",
    "with torch.no_grad():\n",
    "    # --- í…ìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ ---\n",
    "    # output_hidden_states=Trueë¡œ ì„¤ì •í•˜ì—¬ ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ë²¡í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    text_outputs = model(**text_inputs, output_hidden_states=True)\n",
    "    # (Batch, Seq_len, Hidden_dim) -> í‰ê· ì„ ë‚´ì–´ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
    "    text_embedding = text_outputs.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "    # --- ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ ---\n",
    "    # ì´ë¯¸ì§€ë¥¼ LLMì— í†µê³¼ì‹œì¼œ í…ìŠ¤íŠ¸ ê³µê°„ìœ¼ë¡œ íˆ¬ì˜ëœ(projected) ë²¡í„°ë¥¼ ì–»ìŠµë‹ˆë‹¤.\n",
    "    image_outputs = model(**image_inputs, output_hidden_states=True)\n",
    "    image_embedding = image_outputs.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "# 5. ì°¨ì› í™•ì¸ ë° ìœ ì‚¬ë„ ê³„ì‚°\n",
    "print(f\"âœ… Text Embedding Shape: {text_embedding.shape}\")   # ì˜ˆ: [1, 4096]\n",
    "print(f\"âœ… Image Embedding Shape: {image_embedding.shape}\")  # ì˜ˆ: [1, 4096]\n",
    "\n",
    "# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "similarity = F.cosine_similarity(text_embedding, image_embedding)\n",
    "\n",
    "print(f\"\\nğŸ“Š Cosine Similarity between '{text_verb}' and Image:\")\n",
    "print(f\"ğŸ‘‰ Score: {similarity.item():.6f}\")\n",
    "\n",
    "# ë²¡í„° ê³µê°„ ì •ë ¬ í™•ì¸\n",
    "if similarity.item() > 0.1: # ì„ì˜ì˜ ê¸°ì¤€ì \n",
    "    print(\"ğŸ’¡ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ì˜ ì—°ê´€ì„±ì„ ì–´ëŠ ì •ë„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"ğŸ’¡ ìœ ì‚¬ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. (LVLMì€ ìƒì„± ëª©ì ì´ë¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ë‚®ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "962b6993",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimage_outputs\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "image_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb8b72a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen2.5-VL-3B-Instruct'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1b31fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fc16ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197c1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Cosine Similarity ('jump' vs Image): 0.546875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.546875"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
